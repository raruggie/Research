# Ryan Ruggiero

rm(list=ls(all=T)) # clear global env.
gc()

####################### Load packages #######################

library(FactoMineR)
library(factoextra)
library(leaps)
library(glmnet)
library(randomForest)
library(ggcorrplot)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
library(climateR)
library(ggpubr)
library(ggnewscale)
library(ggforce)
library(readxl)
library(MASS)
library(car)
library(CropScapeR)
library(FedData)
library(streamstats)
library(dataRetrieval)
library(sf)
library(raster)
library(terra)
library(kableExtra)
library(sfheaders)
library(mapview)
library(broom)
library(ggsignif)
library(ggpmisc)
library(segmented)
library(readxl)
library(tmap)
library(readxl)
library(zyp)
library(corrr)
library(tidyverse)

sf_use_s2(TRUE) # for sf

setTimeout(1000) # for streamstats api

meters_to_miles = 1/1609.334

####################### Functions #######################

source("Code/Ryan_functions.R")

####################### Goal of code #######################

# 1) Process the NWIS database for the CQ analysis
# 2) Run through CQ metrics/watershed attribute correlation

####################### Workflow #######################

# TP pcode:

pcode<-'00665'

ncode<-'TP'























#### NWIS Query ####

# this code adapted from NYS_site_ranking.R

# Download the metadata for sites with daily flow data. To do this:

# Use dataRetrieval::whatNWISdata to query USGS gauge sites with daily flow data in NY

# df.NWIS.Q_sites<- whatNWISdata(stateCd = 'NY', parameterCd = "00060")

# write.csv(df.NWIS.Q_sites, "C:/PhD/CQ/Raw_Data/df.NWIS.Q_sites.csv", row.names=FALSE)

df.NWIS.Q_sites <- read.csv("C:/PhD/CQ/Raw_Data/df.NWIS.Q_sites.csv", colClasses = c(site_no = "character"))

# clean up this dataframe

# there are duplicates of site numbers with different numbers of daily flow observations
# so group by the site no numbers and just keep the observation with the highest number of flow observations
# I realize now that this is proably not needed since I am grouping by the site number so the resulting number used in the
# raw flow data download will have what it has, but I am just goin to keep this (aka i could just use distinct?)
# also rename some columns and convert them to numeric:
# also filter to just the sites with over 20 nflow days since this is the number restriction on the number of samples:

df.NWIS.Q_sites<-df.NWIS.Q_sites%>%
  group_by(site_no)%>%
  slice(which.max(count_nu))%>%
  rename(latitude = dec_lat_va, longitude = dec_long_va, nflowdays = count_nu, begin_date_flow = begin_date, end_date_flow = end_date)%>%
  mutate(latitude = as.numeric(latitude), longitude = as.numeric(longitude))%>%
  filter(nflowdays>=20)

# NOTE: whatNWISsites(stateCd = "NY", parameterCd = "00060") returns a dataframe with rnow = 1465
# while whatNWISdata(stateCd = "NY", parameterCd = "00060") returns a dataframe with nrow = 2249 ?!?!
# this is because there are duplicates. Using the group_by, slice functions gives a dataframe with
# nrow = 1452 for the whatNWISdata thing.

# From the sites with daily flow data, determine which sites also have over 99 discrete samples from one or more of the constituents of interests. To do this:

# use function I created (sourced) to get a dataframe of sites for just TP with #samples = 20 threshold:

# df.NWIS.TP_sites<-fun.df.Pair_consit_flow(pcode, df.NWIS.Q_sites, n_samples = 20, state = 'NY')

# write.csv(df.NWIS.TP_sites, "Raw_Data/df.NWIS.TP_sites.csv", row.names=FALSE)

df.NWIS.TP_sites<-read.csv("Raw_Data/df.NWIS.TP_sites.csv", colClasses = c(site_no = "character"))

# download the raw daily flow data for these sites. To do this:

# readNWISdv can be used to download a single dataframe with all the raw flow data for a vector of gauge numbers (this takes a looong time):

# df.NWIS.Q<-readNWISdv(siteNumbers = df.NWIS.TP_sites$site_no, parameterCd = '00060', startDate = "", endDate = "", statCd = "00003")

# write.csv(df.NWIS.Q, "Raw_Data/df.NWIS.Q.csv", row.names=FALSE)

df.NWIS.Q<-read.csv("C:/PhD/CQ/Raw_Data/df.NWIS.Q.csv", colClasses = c(site_no = "character"))

# download the raw discrete TP sample data:

# df.NWIS.TP<-readNWISqw(siteNumbers = df.NWIS.TP_sites$site_no, parameterCd = pcode)

# write.csv(df.NWIS.TP, "Raw_Data/df.NWIS.TP.csv", row.names=FALSE)

df.NWIS.TP<-read.csv("Raw_Data/df.NWIS.TP.csv", colClasses = c(site_no = "character"))

#























#### Building CQ df ####

# join the TP with the flow df:

df.NWIS.TP_CQ<-left_join(df.NWIS.TP, df.NWIS.Q, by=c("site_no"="site_no", "sample_dt"="Date"))

# remove observations where there are not CQ pairs:

df.NWIS.TP_CQ<-df.NWIS.TP_CQ%>%drop_na(X_00060_00003)

# take average of multiple samples on the same day at the same site:

df.NWIS.TP_CQ<-df.NWIS.TP_CQ%>%
  group_by(site_no, sample_dt)%>%
  summarise_at(vars(result_va, X_00060_00003), funs(mean(., na.rm=TRUE)))
  
# arrange by number of TP observations. To do this:

# first arrange the dataframe with the number of samples: 

temp<-df.NWIS.TP%>%group_by(site_no)%>%
  summarise(n=n())%>%
  arrange(desc(n))%>%
  mutate(n_sample_rank=rank(-n, ties.method='first'))

# finally merge this df with df.NWIS.TP_CQ and arrange by the new column:

df.NWIS.TP_CQ<-left_join(df.NWIS.TP_CQ,temp, by='site_no')%>%
  arrange(n_sample_rank)

# next step is to use Q yield to get better looking plot... idk if it will help but want to try also doesnt hurt to have the watershed areas as well. To do this:

# download the drainage areas from site metadata using readNWISsite

# df.NWIS.TP_site_metadata<-readNWISsite(siteNumbers = unique(df.NWIS.TP_CQ$site_no))

# write.csv(df.NWIS.TP_site_metadata, "Raw_Data/df.NWIS.TP_site_metadata.csv", row.names=FALSE)

df.NWIS.TP_site_metadata<-read.csv("Raw_Data/df.NWIS.TP_site_metadata.csv", colClasses = c(site_no = "character"))

# then select just the site number and DA column:

df.DA<-df.NWIS.TP_site_metadata%>%
  select(site_no, drain_area_va)

# finally merge with df.NWIS.TP_CQ and create a new Q column with area normalized flows (not worrying about units right now): 
# Note: will filter for NA in C and Q for breakpoint analysis in the next step as to keep the full list of sites with CQ pairs in this dataframe.
# Note: Some sites returned NA on draiange areas in readNWISsite, but I'll delinate anyways so I want the full list:

df.NWIS.TP_CQ<-left_join(df.NWIS.TP_CQ, df.DA, by = 'site_no')%>%
  mutate(Q_yield = X_00060_00003/drain_area_va)

# 137 sites

















































#### Tradeoff matrix ####

# build a matrix of the number of TP samples as a function of watershed size. To do this:

# I already have a df with paired CQ observations and DA, just need to get the distinct sites:

TP_sites<-df.NWIS.TP_CQ%>%distinct(site_no, .keep_all = T)

# set up df to populate:

m<-data.frame(Min_num_samples  = c(20,50,75,100,200), '25' = NA, '50' = NA, '100' = NA, '150'=NA, '250'=NA, '500'=NA, '1000'=NA, 'Unlimited'=NA)

# set up variables for thresholds for number of samples and DA size:

n_sam<- c(20,50,75,100,200)-1

min_DA<- c(25,50,100,150,250,500,1000)

# loop through number of samples (rows):

# i<-1

for (i in seq_along(n_sam)){
  
  temp.i<-TP_sites%>%filter(n>n_sam[i])
  
  m$Unlimited[i]<-dim(temp.i)[1]

  # j<-2
  
  # loop through the size of the DA (columns)
  for(j in  seq_along(min_DA)){
    
    temp.j<-temp.i%>%filter(drain_area_va<=min_DA[j])
    
    m[i,j+1]<-dim(temp.j)[1]
    
  }
  
}

m

#


























#### Apply filters to sites ####

# the first filter is to remove data points and thus potentially entire sites
# that are prior to 2000:

temp1<-df.NWIS.TP_CQ%>%filter(sample_dt >= as.Date('2001-01-10'))

unique(temp1$site_no)

# 82 sites

# but how many of these sites have 20 paired CQ observations?

temp1.1<-temp1%>%
  rename(Name = site_no)%>%
  select(Name, sample_dt,result_va, X_00060_00003)%>%mutate(log_C = log(result_va), log_Q = log(X_00060_00003), C = result_va, Q = X_00060_00003)%>%
  filter(is.finite(log_C))%>%
  filter(is.finite(log_Q))%>%
  group_by(Name)%>%
  summarise(n=n())%>%
  filter(n>=20)

dim(temp1.1)[1] 

# only 69

# reduce temp1 to these sites:

temp1<-temp1%>%filter(site_no %in% temp1.1$Name)

# the next filter is to remove sites that are on long island
# to do this use latitude:

temp2<-filter(df.NWIS.TP_site_metadata, dec_lat_va >40.9364)

# now use this site list to filter down the df.TP_CQ:

temp3<-temp1%>%filter(site_no %in% temp2$site_no)

unique(temp3$site_no)

# 68 sites

# the last filter is gauges 2:

# read in all sheets using function

# l.G2 <- read_excel_allsheets("Raw_Data/gagesII_sept30_2011_conterm.xlsx")

# remove the last element (does notcomtain useful info)

# l.G2[[27]]<-NULL

# and convert to a single df

# df.G2<-reduce(l.G2, full_join, by = "STAID")

# save df.G2:

# save(df.G2, l.G2, file = 'Processed_Data/df.G2.Rdata')

# load df.G2:

load('Processed_Data/df.G2.Rdata')

# filter on G2:

temp4<-temp3%>%filter(site_no %in% df.G2$STAID)

unique(temp4$site_no)

# 42 sites

# save this as the df to move onto future analysis:
# need to alsoremove samples after 2001:

df.TP_CQ<-temp4%>%filter(sample_dt >= as.Date('2001-01-10'))

# map:

# df.NWIS.TP_site_metadata%>%
#   filter(site_no %in% temp4$site_no)%>%
#   st_as_sf(.,coords=c('dec_long_va','dec_lat_va'), crs = 4326, remove = FALSE)%>%
#   mapview(.)

# number of sites in gauges 2:

x<-filter(df.NWIS.TP_site_metadata, site_no %in%df.G2$STAID)

# 89 sites

# number of sites in gauges 2 and not on LI (i.e. without post 2001 filter):

x<-temp2%>%filter(site_no%in% df.G2$STAID)

# 73 sites



















#### Export data ####

# save(df.TP_CQ,file = 'Processed_Data/TP.Rdata')























#### Fitting Breakpoints to CQ curves #### 

# create empty list to hold the results of the segmented function (which does the breakpoint analysis)

l_Seg<-list()

# create a matrix to hold the results of the davies test, which determines if a two slope model is warrented over a single slope model:

davies.test.matrix<-NULL

# create a new dataframe of only paired CQ observations (such that the breakpoit analysis function runs smoothly) (I didnt want to do this in loop for some reason, I cant remeber why but it wouldnt work):

df.TP_CQ_for_BP<-df.TP_CQ%>%
  drop_na(result_va, Q_yield)

# create a vector of ordered unique site names:

temp.loop<-sort(unique(df.TP_CQ_for_BP$site_no))

# test i for for loop building:

# i<-4

# loop through the sites:

for (i in seq_along(temp.loop)){
  
  tryCatch({
    
    # print the site name for loop debugging:
    
    print(i)
    print(temp.loop[i])
    
    # create a dataframe that will work with segmented. To do this: 
    # filter for the site the loop is in
    # add log transformed C and Q columns, as well as duplicated columns for renamed C and Q
    # filter for real log C and Q values so breakpoint analysis works smoothly:
    
    df<-df.TP_CQ_for_BP%>%
      filter(site_no == temp.loop[i])%>%
      mutate(log_C = log(result_va), log_Q = log(X_00060_00003), C = result_va, Q = X_00060_00003)%>%
      filter(is.finite(log_C))%>%
      filter(is.finite(log_Q))
    
    # build a single slope lm for log C and Q. Tis model is also used inthe breakpoint analysis inthenext step:
    
    m<-lm(log_C~log_Q, df)
    
    # perform breakpoint regression:
    
    m_seg<-segmented(obj = m, npsi = 1)
    
    # perform davies test for constant linear predictor:
    # the results are saved as a string with the the site name and true/false:
    
    x<-paste(temp.loop[i], '-', davies.test(m)$p.val<0.05)
    
    # add the results of davies test to the matrix made prior to this for loop:
    
    davies.test.matrix<-c(davies.test.matrix,x)
    
    # save the breakpoints
    
    bp<-m_seg$psi[1]
    
    # save the slopes: To do this:
    # a conditional statement is needed since sometimes the segmented function wont fit a two slope model at all and will return a object that doesnt work with the slope function used here:
    
    if(length(class(m_seg))==2){
      s<-as.data.frame(slope(m_seg))
    } else{
      s<-NA
    }
    
    # get the intercepts (again conditional statement is needed):
    
    if(length(class(m_seg))==2){
      inter<-as.data.frame(intercept(m_seg))
    } else{
      inter<-NA
    }
    
    
    # get the model fitted data and put in a dataframe:
    
    fit <- data.frame(Q = df$log_Q, Seg_C = fitted(m_seg))
    
    # reformat this dataframe to export out of the loop
    
    if(length(class(m_seg))==2){
      result_df<-fit%>%mutate(site = temp.loop[i], Date = df$sample_dt, n = df$n, Q_real = df$Q, C = df$C, I1 = inter$Est.[1], I2 = inter$Est.[2], Slope1 = s[1,1], Slope2 = s[2,1], BP = bp)
    } else{
      result_df<-fit%>%mutate(site = temp.loop[i], Date = df$sample_dt, n = df$n, Q_real = df$Q, C = df$C, I1 = NA, I2 = NA, Slope1 = NA, Slope2 = NA, BP = NA)
    }
    
    l_Seg[[i]]<-result_df
    
  }, 
  
  error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
  
}

# Looking at the davies test results:

davies.test.matrix

# transform this matrix into a two column dataframe for use later with df_Seg and plotting. To do this:
# separate the matrix into two columns
# use mutate to remove white space around these character columns:

df.davies<-as.data.frame(davies.test.matrix)%>%
  separate_wider_delim(1, "-", names = c("site", "BP_yes"))%>%
  mutate(across(c(1,2), trimws))

# now combine the list of dfs of the breakpoint analysis results (with fitted values, intercepts and slopes) into a single df:

df_Seg<-bind_rows(l_Seg)

# merge this dataframe with the davies test result dfs to add the BP_yes column, use replace and the BP yes column with a conditional statement to set the breakpoint Q and C column rows to NA, as to not plot the segmeneted line if davies test was false:

df_Seg<-df_Seg%>%
  left_join(., df.davies, by = 'site')%>%
  mutate(across(c(1,2), ~replace(., BP_yes == 'FALSE', NA)))

# add a column for number of samples:

df_Seg<-left_join(df_Seg, temp[,c(1,3)], by = c('site'='site_no'))%>%
  arrange(n_sample_rank)

# ready to plot:

# p<-ggplot(df_Seg, aes(x = log(Q_real), y = log(C)))+
#   geom_point()+
#   geom_smooth(method = 'lm')+
#   geom_line(aes(x = Q, y = Seg_C), color = 'tomato')+
#   facet_wrap(dplyr::vars(n_sample_rank), scales = 'free')+
#   theme(
#     strip.background = element_blank(),
#     strip.text.x = element_blank()
#   )

# p

# one last thing: lets look at a map of these:

# map.NWIS.TP_sites<-df.NWIS.TP_site_metadata%>%
#   rename(longitude=8,latitude=7)%>%
#   drop_na(latitude,longitude)%>%
#   st_as_sf(.,coords=c('longitude','latitude'), crs = 4326)%>%
#   mutate(site_id= paste(site_no, station_nm), n = NA, .before = 1)%>%
#   select(c(1:3))

# mapview(map.NWIS.TP_sites)

# save(map.NWIS.TP_sites, file = 'C:/PhD/CQ/Processed_Data/map.NWIS.TP_sites.Rdata')























#### Calculating Average Annual Consituent Yield ####

# (for use in correlaitons and MLR)

# step 1. calcualte average annual hydrograph for each site. to do this:

# filter the daily flow records to just the sites used:

l.avg_ann_hydro<-df.NWIS.Q%>%filter(site_no %in% df.TP_CQ$site_no)

# split the daily flow records into list of dfs

l.avg_ann_hydro<-split(l.avg_ann_hydro, f = l.avg_ann_hydro$site_no)

# strip out year from date (just converts it to 2023) for each dataframe, then group by and summarize to get mean annual flow for each day of calendar year:

l.avg_ann_hydro<-lapply(l.avg_ann_hydro, \(i) i%>%mutate(Date = as.Date(Date))%>%mutate(year = year(Date), Date = as.Date(format(Date, "%m-%d"), "%m-%d"))%>%group_by(Date)%>%summarize(mean_Q = mean(X_00060_00003, na.rm= T)))

# step 2. predict C for each of day of the year using the mean flow rate for each site in the list:todothis:

# create a list of linear model objects for each sites CQ relaitonship:

l.C_daily<-df.TP_CQ%>%
  rename(Name = site_no)%>%
  select(Name, sample_dt,result_va, X_00060_00003)%>%mutate(log_C = log(result_va), log_Q = log(X_00060_00003), C = result_va, Q = X_00060_00003)%>%
  filter(is.finite(log_C))%>%
  filter(is.finite(log_Q))%>%
  split(., f =.$Name)%>%
  lapply(., \(i) lm(log_C ~ log_Q, i))

# make sure the order of the sites in the flow and lm lists are the same:

names(l.C_daily)==names(l.avg_ann_hydro)

# use this list to create list of bias correction factors:

l.BCF<-lapply(l.C_daily, \(i) exp((summary(i)$sigma^2)/2))

# predict the concentration for each day for each site: this predicts the concnetration in log space, so transforming into real space here as well using BCF:

l.C_daily<-lapply(names(l.C_daily), \(i) l.BCF[[i]]*exp(as.numeric(predict(l.C_daily[[i]], data.frame(log_Q = log(l.avg_ann_hydro[[i]]$mean_Q) ) ) ) ) )%>%purrr::set_names(names(l.avg_ann_hydro))

# step 3. multiply predicted average daily C by daily Q to get daily load, sum up, then convert to yield. To do this:

# save just the daily Q:

l.Q.temp<-lapply(l.avg_ann_hydro, \(i) i$mean_Q)

# set up list of drainage areas:

l.DA<-readNWISsite(siteNumbers = names(l.C_daily))%>%select(site_no, drain_area_va)%>%split(., .$site_no)%>%lapply(., \(i) i$drain_area_va)

# multiply daily C by daily Q and convert units:
# mg/L * ft^3/sec * 28.3168 L/ft3 * 86400 sec /day * 1g/1000mg * 1kg/1000g * 1/mi^2 * 1mi^2/258.999 ha = kg/ha/day

unit_conversion=28.3168*86400*(1/1000)*(1/1000)*(1/258.999)

l.Yield<-lapply(names(l.C_daily), \(i) l.C_daily[[i]]*l.Q.temp[[i]]*(1/l.DA[[i]])*unit_conversion)%>%purrr::set_names(names(l.avg_ann_hydro))

# sum up for the year and convert to df

df.Yield <- lapply(l.Yield, sum)%>%dplyr::bind_rows(., .id = 'Name')%>%pivot_longer(cols = everything(), names_to = 'Name', values_to = 'Yield') # units of kg/ha/year

#

#### Calculating Average Annual Consituent Yield for Top 50 ####

# (for use in correlaitons and MLR)

# step 1 from the previous section can be used here

# step 2 needs to incorporate two linear models, bottom and top 50:

l.C_daily.bottom50<-split(df.TP_CQ, f = df.TP_CQ$site_no)%>% # split the df into list of dfs for each site, 
  lapply(., \(i) i%>%filter(X_00060_00003<median(i$X_00060_00003)))%>% # filter to the median flow rate for each site,
  bind_rows(.)%>% # bind back into single dataframe:
  rename(Name = site_no)%>%
  select(Name, sample_dt,result_va, X_00060_00003)%>%
  mutate(log_C = log(result_va), log_Q = log(X_00060_00003), C = result_va, Q = X_00060_00003)%>%
  filter(is.finite(log_C))%>%
  filter(is.finite(log_Q))%>%
  split(., f =.$Name)%>%
  lapply(., \(i) lm(log_C ~ log_Q, i))

l.C_daily.top50<-split(df.TP_CQ, f = df.TP_CQ$site_no)%>% # split the df into list of dfs for each site, 
  lapply(., \(i) i%>%filter(X_00060_00003>=median(i$X_00060_00003)))%>% # filter to the median flow rate for each site,
  bind_rows(.)%>% # bind back into single dataframe:
  rename(Name = site_no)%>%
  select(Name, sample_dt,result_va, X_00060_00003)%>%
  mutate(log_C = log(result_va), log_Q = log(X_00060_00003), C = result_va, Q = X_00060_00003)%>%
  filter(is.finite(log_C))%>%
  filter(is.finite(log_Q))%>%
  split(., f =.$Name)%>%
  lapply(., \(i) lm(log_C ~ log_Q, i))

# make sure the order of the sites in the flow and lm lists are the same:

names(l.C_daily.bottom50)==names(l.avg_ann_hydro)
names(l.C_daily.top50)==names(l.avg_ann_hydro)

# use this list to create list of bias correction factors:

l.BCF.bottom50<-lapply(l.C_daily.bottom50, \(i) exp((summary(i)$sigma^2)/2))
l.BCF.top50<-lapply(l.C_daily.top50, \(i) exp((summary(i)$sigma^2)/2))

# create df of median Q for each site:

df.median.Q<-df.TP_CQ%>%group_by(site_no)%>%summarise(median_Q=median(X_00060_00003))

# predict the concentration for each day for each site: 
# ifelse is used to determine if the average daily flow is less than median, use the model in bottom50, if equal to or above median use model in top50
# this predicts the concnetration in log space, so transforming into real space here as well using BCF:

l.C_daily<-lapply(seq_along(l.avg_ann_hydro), \(i) l.avg_ann_hydro[[i]]%>%mutate(C = ifelse(mean_Q < df.median.Q$median_Q[i], 
                                                                                            l.BCF.bottom50[[i]]*exp(as.numeric(predict(l.C_daily.bottom50[[i]], data.frame(log_Q = log(l.avg_ann_hydro[[i]]$mean_Q))))),
                                                                                            l.BCF.top50[[i]]*exp(as.numeric(predict(l.C_daily.top50[[i]], data.frame(log_Q = log(l.avg_ann_hydro[[i]]$mean_Q)))))
                                                                                            )))%>%purrr::set_names(names(l.avg_ann_hydro))

# I think this worked!! cause if I sub in NA for the else line in mutate, those rows are NA!

# step 3. multiply predicted average daily C by daily Q to get daily load, sum up, then convert to yield. To do this:

# set up list of vectors of drainage areas:

l.DA<-readNWISsite(siteNumbers = names(l.C_daily))%>%select(site_no, drain_area_va)%>%split(., .$site_no)%>%lapply(., \(i) i$drain_area_va)

# multiply daily C by daily Q and convert units:
# mg/L * ft^3/sec * 28.3168 L/ft3 * 86400 sec /day * 1g/1000mg * 1kg/1000g * 1/mi^2 * 1mi^2/258.999 ha = kg/ha/day

unit_conversion=28.3168*86400*(1/1000)*(1/1000)*(1/258.999)

l.Yield<-lapply(names(l.C_daily), \(i) l.C_daily[[i]]$C*l.Q.temp[[i]]*(1/l.DA[[i]])*unit_conversion)%>%purrr::set_names(names(l.avg_ann_hydro))

# sum up for the year and convert to df

df.Yield.2model <- lapply(l.Yield, sum)%>%dplyr::bind_rows(., .id = 'Name')%>%pivot_longer(cols = everything(), names_to = 'Name', values_to = 'Yield') # units of kg/ha/year

#














#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#### Correlations ####
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

# reduce the predictor set down to those we believe are defensible:

# I also went in to the variable description excel sheet and isolated these variables as well:

temp1<-read.csv('Processed_Data/G2_pred_to_keep_2.csv')[,1]

# combining these with the vairbales in entire sheets as well:

pred_to_keep<-c("HYDRO_DISTURB_INDX", 
               "pre1990_STOR", 
               names(l.G2$Landscape_Pat), 
               names(l.G2$LC06_Basin),
               names(l.G2$LC_Crops),
               temp1,
               # 'HGA', 'HGB', 'HGC', 'HGD',
               "ELEV_MEDIAN_M_BASIN",
               "ELEV_STD_M_BASIN",
               "RRMEDIAN"
)

# removing 'STADID':

pred_to_keep <- pred_to_keep[!(pred_to_keep %in% "STAID")]

# removing the duplicates of DEV and PDEN, and also CDL_WWHT_SOY_DBL_CROP

pred_to_keep <- pred_to_keep[!(pred_to_keep %in% c("DEVOPENNLCD06","DEVLOWNLCD06","DEVMEDNLCD06","DEVHINLCD06", "PDEN_DAY_LANDSCAN_2007", "PDEN_NIGHT_LANDSCAN_2007", "CDL_WWHT_SOY_DBL_CROP"))]

# filtering df.G2 to this predictor list and sites:

df.G2.reduced<-df.G2%>%
  select(c('STAID', pred_to_keep, starts_with('HG')))%>%
  filter(STAID %in% df.TP_CQ$site_no)

# take the average of the RIP and MAIN for each land use:

# df.G2.reduced<-df.G2.reduced%>%
#   rowwise() %>%
#   mutate(MAIN_RIP_DEV_avg = mean(c_across(starts_with("MAINS") | starts_with("RIP") & ends_with("DEV")), na.rm = TRUE),
#          MAIN_RIP_FOREST_avg = mean(c_across(starts_with("MAINS") | starts_with("RIP") & ends_with("FOREST")), na.rm = TRUE),
#          MAIN_RIP_PLANT_avg = mean(c_across(starts_with("MAINS") | starts_with("RIP") & ends_with("PLANT")), na.rm = TRUE),
#          .keep = 'unused'
#          )

# adding up the other crops:

df.G2.reduced<-df.G2.reduced%>%
  mutate(sum_of_misc_crops = sum(c_across(ends_with(c("COTTON", "RICE", "SORGHUM", "SUNFLOWERS", "PEANUTS", "BARLEY", "DURUM_WHEAT", "OATS", "DRY_BEANS", "POTATOES", "ORANGES", 'OTHER_CROPS', 'IDLE'))), na.rm = TRUE), .keep = 'unused')

names(df.G2.reduced)

# export this predictor set for the 42 sites:

save(df.G2.reduced, file = 'Processed_Data/df.G2.reduced.Rdata')

# combined the reduced predictors df with the already filtered CQ data:

df.Correlation<-df.TP_CQ%>%
  rename(Name = site_no)%>%
  select(Name, sample_dt,result_va, X_00060_00003)%>%mutate(log_C = log(result_va), log_Q = log(X_00060_00003), C = result_va, Q = X_00060_00003)%>%
  filter(is.finite(log_C))%>%
  filter(is.finite(log_Q))%>%
  left_join(., df.G2.reduced, by = c('Name'='STAID'))

# create a dataframe with OLS and sens slope intercept and slopes:

df.OLS<-df.Correlation%>%
  group_by(Name)%>%
  do({ OLS.co <- coef(lm(log_C ~ log_Q, .))
  summarize(., OLS.I = OLS.co[1], 
            OLS.S = OLS.co[2])
  }) %>%
  ungroup

df.Sens<-df.Correlation%>%
  group_by(Name)%>%
  do({ Sens.co<-zyp.sen(log_C~log_Q,.)
  summarize(., Sen.I = Sens.co$coefficients[[1]],
            Sen.S= Sens.co$coefficients[[2]])
  }) %>%
  ungroup

# merge OLS and Sens:

df.OLS_Sens<-left_join(df.OLS, df.Sens, by = 'Name')

# merge constiuent yield:

df.OLS_Sens<-left_join(df.OLS_Sens, df.Yield, by = 'Name')

# merge back the watershed characteristic data to this dataframe:

df.OLS_Sens<-left_join(df.OLS_Sens, df.G2.reduced, by = c('Name'='STAID'))

# now run correlations between intercepts and slopes and watershed characteristics. to do this: (I orginally did this workflow using n_months (C:\PhD\Research\Mohawk\Code\Mohawk_Regression-analyizing_predictor_df.R)

# set up variable for number of sites:

n_sites<-dim(df.OLS_Sens)[1] 

# use the corrr package to correlate() and focus() on your variable of choice.

df.cor <- df.OLS_Sens %>% 
  correlate(method = 'spearman') %>%
  focus(c(OLS.I, OLS.S, Sen.I, Sen.S, Yield))%>%
  pivot_longer(cols= c(2:6), names_to = 'CQ_Parameter', values_to = 'Spearman_Correlation')%>%
  mutate(p_val = round(2*pt(-abs(Spearman_Correlation*sqrt((n_sites-2)/(1-(Spearman_Correlation)^2))), n_sites-2),2))%>%
  mutate(sig_0.05 = ifelse(p_val <= 0.05, 'sig', 'not'))%>%
  drop_na(p_val) # some standard deviaitons return NA because the watershed characteristic values are zero

# I also want to try standardizing the predictors:

df.cor.0to1<-df.OLS_Sens %>% 
  mutate(across(7:last_col(), ~ (.-min(.))/(max(.)-min(.))))%>% # could add term: .names = "{.col}_standarized")
  correlate(method = 'spearman') %>%
  focus(c(OLS.I, OLS.S, Sen.I, Sen.S, Yield))%>%
  pivot_longer(cols= c(2:6), names_to = 'CQ_Parameter', values_to = 'Spearman_Correlation')%>%
  mutate(p_val = round(2*pt(-abs(Spearman_Correlation*sqrt((n_sites-2)/(1-(Spearman_Correlation)^2))), n_sites-2),2))%>%
  mutate(sig_0.05 = ifelse(p_val <= 0.05, 'sig', 'not'))%>%
  drop_na(p_val) # some standard deviaitons return NA because the watershed characteristic values are zero

# looking at the regualr and 0 to 1 standarized spearman correlaitons, they are the same

df.cor$Spearman_Correlation==df.cor.0to1$Spearman_Correlation

# so not going to proceed with 0 to 1

# then plot results: to do this:

# create a list of each CQ parameter (4: OLS and Sens slope and intercept)and format it for ggplotting:

l.cor<-df.cor %>%
  split(., df.cor$CQ_Parameter)%>%
  lapply(., \(i) i%>% 
           arrange(Spearman_Correlation)%>%  
           slice_head(n = 7)%>%
           bind_rows(i%>%arrange(desc(Spearman_Correlation))%>%slice_head(n = 7))%>%
           mutate(sig_0.05 = factor(sig_0.05, levels = c('not', 'sig')))%>%
           mutate(term = factor(term, levels = unique(term[order(Spearman_Correlation)])))%>%
           filter(!between(Spearman_Correlation, -0.25,.25))) # Order by correlation strength

# make plot list using lapply:

plist<-lapply(l.cor, \(i) i%>%ggplot(aes(x = term, y = Spearman_Correlation, color = sig_0.05)) +
                geom_bar(stat = "identity") +
                scale_color_manual(values = c("not" = "red", "sig" = "blue"),na.value = NA, drop = FALSE)+
                facet_wrap('CQ_Parameter')+
                ylab(paste('Spearman Correlation')) +
                xlab("Watershed Attribute")+
                theme(axis.text.x=element_text(angle=40,hjust=1))+
                theme(legend.position="bottom"))

# arrange plots:

p1<-ggpubr::ggarrange(plotlist = plist, ncol=3, nrow=2, common.legend = TRUE, legend="bottom")

# add title:

p1<-annotate_figure(p1, top = text_grob(paste(ncode, "Correlated Against Gauges 2"),
                                        color = "red", face = "bold", size = 14))
# plot:

p1

# save(p1, file = 'Processed_Data/p1.Rdata')

# univariate plots of the top correlates/small land use types:

# Lets do plots of OLS intercept: to do this:

# determine the top correlates (the numberof the list determined which CQ parameter)
# here 1 = OLS intercept:

OLS<-l.cor[[1]]%>%arrange(desc(Spearman_Correlation))

# make univariate plots (facets) of Y (determined in [[]]) above and predictors: 
# note the predictor column isturned into an ordered factor based on thespearman correlation values
# to makethe facet plots in order:

df.OLS_Sens%>%
  select(Name, OLS$CQ_Parameter[1], OLS$term)%>%
  pivot_longer(cols = c(3:last_col()), names_to = 'Type', values_to = 'Value')%>%
  drop_na(Value)%>%
  # mutate_if(is.numeric, ~replace(., . == 0, NA))%>%
  left_join(., OLS%>%select(term, Spearman_Correlation), by = c('Type'='term'))%>%
  mutate(Type = factor(Type, levels=unique(Type[order(-Spearman_Correlation,Type)]), ordered=TRUE))%>%
  ggplot(., aes(x = Value, y = !!sym(OLS$CQ_Parameter[1])))+
  geom_smooth(method = 'lm')+
  geom_point()+
  facet_wrap('Type', scales = 'free')+
  ggtitle(paste('Univariate plots of', OLS$CQ_Parameter[1], 'against Top Gauges 2 Correlates for', OLS$CQ_Parameter[1], 'ordered from highest to lowest Spearman Rank Correlation' ))

# create a function to use lapply to plot all 4 univariate sets:

# number<-5

# make_plot<-function(number){
#   
#   OLS<-l.cor[[number]]%>%arrange(desc(Spearman_Correlation))
#   
#   x<-df.OLS_Sens%>%
#     select(Name, OLS$CQ_Parameter[1], OLS$term)%>%
#     pivot_longer(cols = c(3:last_col()), names_to = 'Type', values_to = 'Value')%>%
#     drop_na(Value)%>%
#     # mutate_if(is.numeric, ~replace(., . == 0, NA))%>%
#     left_join(., OLS%>%select(term, Spearman_Correlation), by = c('Type'='term'))%>%
#     mutate(Type = factor(Type, levels=unique(Type[order(-Spearman_Correlation,Type)]), ordered=TRUE))%>%
#     ggplot(., aes(x = Value, y = !!sym(OLS$CQ_Parameter[1])))+
#     geom_smooth(method = 'lm')+
#     geom_point()+
#     facet_wrap('Type', scales = 'free')+
#     ggtitle(paste('Univariate plots of', OLS$CQ_Parameter[1], 'against Top Gauges 2 Correlates for', OLS$CQ_Parameter[1], 'ordered from highest to lowest Spearman Rank Correlation' ))
#   
#   x
#   
# }

# use function in lapply (clear plot list first):

# lapply(c(1:5), \(i) make_plot(i))

#


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#### Correlations with top50 ####
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

# determine the median flow rate for each sites CQ observations: to do this:

df.TP_CQ.top50<-split(df.TP_CQ, f = df.TP_CQ$site_no)%>% # split the df into list of dfs for each site, 
  lapply(., \(i) i%>%filter(X_00060_00003>median(i$X_00060_00003)))%>% # filter to the median flow rate for each site,
  bind_rows(.) # bind back into single dataframe:

# now run through the Correlations workflow for the top50 OLS model:

df.Correlation.top50<-df.TP_CQ.top50%>%
  rename(Name = site_no)%>%
  select(Name, sample_dt,result_va, X_00060_00003)%>%mutate(log_C = log(result_va), log_Q = log(X_00060_00003), C = result_va, Q = X_00060_00003)%>%
  filter(is.finite(log_C))%>%
  filter(is.finite(log_Q))%>%
  left_join(., df.G2.reduced, by = c('Name'='STAID'))
df.OLS.top50<-df.Correlation.top50%>% # create a dataframe with OLS and sens slope intercept and slopes:
  group_by(Name)%>%
  do({ OLS.co <- coef(lm(log_C ~ log_Q, .))
  summarize(., OLS.I = OLS.co[1], 
            OLS.S = OLS.co[2])
  }) %>%
  ungroup
df.Sens.top50<-df.Correlation.top50%>%
  group_by(Name)%>%
  do({ Sens.co<-zyp.sen(log_C~log_Q,.)
  summarize(., Sen.I = Sens.co$coefficients[[1]],
            Sen.S= Sens.co$coefficients[[2]])
  }) %>%
  ungroup
df.OLS_Sens.top50<-left_join(df.OLS.top50, df.Sens.top50, by = 'Name') # merge OLS and Sens:
# here is where top 50 workflow differes: need to use the recalcualted df.Yield using two OLS models 
df.OLS_Sens.top50<-left_join(df.OLS_Sens.top50, df.Yield.2model, by = 'Name') # merge constiuent yield:
df.OLS_Sens.top50<-left_join(df.OLS_Sens.top50, df.G2.reduced, by = c('Name'='STAID')) # merge back the watershed characteristic data to this dataframe:
n_sites<-dim(df.OLS_Sens.top50)[1] # set up variable for number of sites:
df.cor.top50 <- df.OLS_Sens.top50 %>% # use the corrr package to correlate() and focus() on your variable of choice.
  correlate(method = 'spearman') %>%
  focus(c(OLS.I, OLS.S, Sen.I, Sen.S, Yield))%>%
  pivot_longer(cols= c(2:6), names_to = 'CQ_Parameter', values_to = 'Spearman_Correlation')%>%
  mutate(p_val = round(2*pt(-abs(Spearman_Correlation*sqrt((n_sites-2)/(1-(Spearman_Correlation)^2))), n_sites-2),2))%>%
  mutate(sig_0.05 = ifelse(p_val <= 0.05, 'sig', 'not'))%>%
  drop_na(p_val) # some standard deviaitons return NA because the watershed characteristic values are zero
l.cor.top50<-df.cor.top50 %>% # create a list of each CQ parameter (4: OLS and Sens slope and intercept)and format it for ggplotting:
  split(., df.cor.top50$CQ_Parameter)%>%
  lapply(., \(i) i%>% 
           arrange(Spearman_Correlation)%>%  
           slice_head(n = 7)%>%
           bind_rows(i%>%arrange(desc(Spearman_Correlation))%>%slice_head(n = 7))%>%
           mutate(sig_0.05 = factor(sig_0.05, levels = c('not', 'sig')))%>%
           mutate(term = factor(term, levels = unique(term[order(Spearman_Correlation)])))%>%
           filter(!between(Spearman_Correlation, -0.25,.25))) # Order by correlation strength
# note*: need to add thisstep for top50 workflow:
# when removing CDL_WWHT_SOY_DBL_CROP from pred to keep (it is clearly not good to include when looing at the univariate plots)
# OLS.S and SEN.S have no correlates between -0.25 and 0.25
# thus I need toremove those from l.cor:
l.cor.top50<-purrr::keep(l.cor.top50, ~ nrow(.x) > 0)
plist<-lapply(l.cor.top50, \(i) i%>%ggplot(aes(x = term, y = Spearman_Correlation, color = sig_0.05)) + # make plot list using lapply:
                geom_bar(stat = "identity") +
                scale_color_manual(values = c("not" = "red", "sig" = "blue"),na.value = NA, drop = FALSE)+
                facet_wrap('CQ_Parameter')+
                ylab(paste('Spearman Correlation')) +
                xlab("Watershed Attribute")+
                theme(axis.text.x=element_text(angle=40,hjust=1))+
                theme(legend.position="bottom"))
p1<-ggpubr::ggarrange(plotlist = plist, ncol=2, nrow=2, common.legend = TRUE, legend="bottom") # arrange plots:
p1<-annotate_figure(p1, top = text_grob(paste(ncode, "Correlated Against Gauges 2"), color = "red", face = "bold", size = 14)) # add plot title:
p1 # plot:

# look at univarite plot for AANY:

OLS<-l.cor.top50[[3]]%>%arrange(desc(Spearman_Correlation))

# make univariate plots (facets) of Y (determined in [[]]) above and predictors: 
# note the predictor column isturned into an ordered factor based on thespearman correlation values
# to makethe facet plots in order:

df.OLS_Sens.top50%>%
  select(Name, OLS$CQ_Parameter[1], OLS$term)%>%
  pivot_longer(cols = c(3:last_col()), names_to = 'Type', values_to = 'Value')%>%
  drop_na(Value)%>%
  # mutate_if(is.numeric, ~replace(., . == 0, NA))%>%
  left_join(., OLS%>%select(term, Spearman_Correlation), by = c('Type'='term'))%>%
  mutate(Type = factor(Type, levels=unique(Type[order(-Spearman_Correlation,Type)]), ordered=TRUE))%>%
  ggplot(., aes(x = Value, y = !!sym(OLS$CQ_Parameter[1])))+
  geom_smooth(method = 'lm')+
  geom_point()+
  # scale_x_log10(
  #   breaks = scales::trans_breaks("log10", function(x) 10^x),
  #   labels = scales::trans_format("log10", scales::math_format(10^.x))
  # )+
  # scale_y_log10(
  #   breaks = scales::trans_breaks("log10", function(x) 10^x),
  #   labels = scales::trans_format("log10", scales::math_format(10^.x))
  # )+
  facet_wrap('Type', scales = 'free')+
  ggtitle(paste('Univariate plots of', OLS$CQ_Parameter[1], 'against Top Gauges 2 Correlates for', OLS$CQ_Parameter[1], 'ordered from highest to lowest Spearman Rank Correlation' ))

#


















#### Categorizing land use ####

# USGS criteria:
# Agricultural sites have >50% agricultural land and ≤5% urban land;
# urban sites have >25% urban and ≤25% agricultural land; 
# undeveloped sites have ≤ 5% urban and ≤ 25% agricultural land; 
# all other combinations of urban, agricultural, and undeveloped lands are classified as mixed

# initate a dataframe with the landuse values from gauges 2:

df.NLCD06<-distinct(df.OLS_Sens, Name, .keep_all = T)

# I confirmed that pasture + crops = plant:

(df.NLCD06$PASTURENLCD06+df.NLCD06$CROPSNLCD06)==df.NLCD06$PLANTNLCD06

# and that the ones that start with DEV sum up to the first DEV one.

# create the land use class column based on USGS critiera:
# adjusted thresholds:

df.NLCD06<-df.NLCD06%>%
  mutate(USGS.LU.Adjusted = case_when(.default = 'Mixed',
                                      PLANTNLCD06 > 30 & DEVNLCD06 <= 10 ~ 'Agriculture',
                                      DEVNLCD06 > 10 & PLANTNLCD06 <= 30 ~ 'Urban',
                                      DEVNLCD06+PLANTNLCD06 <= 10 ~ 'Undeveloped'))

# frequency table:

table(df.NLCD06$USGS.LU.Adjusted)

# boxplots of

p<-df.NLCD06%>%
  pivot_longer(cols = 2:5, names_to = 'CQ_parameter', values_to = 'Value')%>%
  ggplot(., aes(x=USGS.LU.Adjusted, y=Value, color =USGS.LU.Adjusted ))+
  geom_boxplot(varwidth = TRUE, alpha=0.2)+
  # scale_x_discrete(labels=my_xlab)+
  facet_wrap('CQ_parameter', scales = 'free')+
  stat_compare_means(method = "anova")+      # Add global p-value # , label.y = max(df.datalayers$Value) ???
  stat_compare_means(label = "p.signif", method = "t.test",
                     ref.group = "0.5") +
  ggtitle('Adjusted USGS Thresholds using Aggregated Data Layers')

# p

#







































#### Grouping CQ curves (stationary, mobilization, dilutionary, complex) ####

# create a list of dataframes for each sites CQ observations:

df.TP_CQ<-df.TP_CQ%>%
  rename(Name = site_no)%>%
  mutate(log_C = log(result_va), log_Q = log(X_00060_00003), C = result_va, Q = X_00060_00003)%>%
  filter(is.finite(log_C))%>%
  filter(is.finite(log_Q))

l.TP_CQ<-df.TP_CQ%>%
  split(., .$Name)

# create lm models for each site:

l.TP_CQ.lm<-lapply(l.TP_CQ, \(i) lm(log_C~log_Q, data=i))

# save the model coef ad pvals:

TP_CQ.coef<-as.data.frame(bind_rows(lapply(l.TP_CQ.lm, \(i) summary(i)$coefficients[,1]), .id = 'site_no'))%>%
  rename(Intercept = 2, Slope = 3)

# save the pvalues 

TP_CQ.pvals<-as.data.frame(bind_rows(lapply(l.TP_CQ.lm, \(i) summary(i)$coefficients[,4]), .id = 'site_no'))%>%
  rename(Intercept.pval = 2, Slope.pval = 3)

# merge the two dfs:

df.lm<-left_join(TP_CQ.coef,TP_CQ.pvals,by='site_no')%>%left_join(., df.TP_CQ%>%select(Name, n_sample_rank)%>%distinct(., .keep_all = T), by = c('site_no' = 'Name'))

# add column for CQ type:

df.lm<-mutate(df.lm, Type = ifelse(Slope.pval>0.05, 'Stationary', ifelse(Slope>0, 'Mobilization', 'Dilution')))

# merge CQ type labels to the df for plotting

df.TP_CQ<-left_join(df.TP_CQ,df.lm%>%select(site_no, Type),by=c('Name'='site_no'))

# create a df for CQplot of all sites with BP analysis: make sure to filter out pre 2001 samples:

df_Seg.2<-filter(df_Seg, site %in% df.lm$site_no)%>%
  left_join(.,df.lm%>%select(site_no, Type),by=c('site'='site_no'))

# make plot:

p<-ggplot(df_Seg.2, aes(x = log(Q_real), y = log(C)))+
  geom_point(aes(color = Type))+
  geom_smooth(method = 'lm')+
  geom_line(aes(x = Q, y = Seg_C), color = 'purple', size = 2)+
  facet_wrap('n_sample_rank', scales = 'free')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )

# p

# looking at this plot I want to add a fourth CQ type for complex, if the slopes of the BP analysis look widely different. 
# I will start with calcuating the angle between pre-post BP slope and see which sites get signled out:

# add a new column with the angle between the two lines:

df_Seg.2<-df_Seg.2%>%
  mutate(slope_angle=factor(round(atan(abs((Slope2-Slope1)/(1+(Slope2*Slope1)))),1)))

# I want to add land use as color as well. Merge the plotting df with the land use df:

df_Seg.2<-left_join(df_Seg.2, df.NLCD06%>%select(Name, USGS.LU.Adjusted), by = c('site'='Name'))

# create color pallete for the slope angle:

hc<-heat.colors(length(unique(df_Seg.2$slope_angle)), rev = T)

# make plot:

p<-ggplot(df_Seg.2, aes(x = log(Q_real), y = log(C)))+
  geom_point(aes(color = Type))+
  scale_color_manual(name = "CQ Type", values = c("red", "blue", "green"))+
  ylab('log(TP)')+
  geom_smooth(method = 'lm')+
  new_scale_color() +
  geom_line(aes(x = Q, y = Seg_C), size = 2.5, color = 'black')+
  geom_line(aes(x = Q, y = Seg_C, color = slope_angle), size = 2)+
  scale_color_manual(name = "Slope Angle", values = hc)+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'fixed')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )+
  geom_rect(data = df_Seg.2%>%distinct(df_Seg.2$site, .keep_all = T), aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, fill = USGS.LU.Adjusted), alpha = .35)+
  scale_fill_manual(name = "USGS Landuse\n(Adjusted)", values = c("red", "blue","yellow", "green"))

# p

# export df_Seg.2:

# save(df_Seg.2, file = 'Processed_Data/TP.df_Seg.2.Rdata')

# based on this plot, I think I need to zoom in on each one:

# create a plotting function for each site:

# p.fun<-function(df){
#   ggplot(df, aes(x = log(Q_real), y = log(C)))+
#     geom_point(aes(color = Type))+
#     scale_color_manual(name = "CQ Type", values = c("red", "blue", "green"))+
#     geom_smooth(method = 'lm')+
#     new_scale_color() +
#     geom_line(aes(x = Q, y = Seg_C), size = 2.5, color = 'black')+
#     geom_line(aes(x = Q, y = Seg_C, color = slope_angle), size = 2)+
#     scale_color_manual(name = "Slope Angle", values = hc)+
#     ggtitle(df_Seg.2$site[df_Seg.2$n_sample_rank==i])
# }

# use function in lappy to make lots of plots (clear plot list first)

# lapply(sort(unique(df_Seg.2$n_sample_rank)), \(i) df_Seg.2%>%filter(n_sample_rank==i)%>%p.fun(.))

# now looking at the expanded plots for each site, I feel that
# non really could be justified as complex! it feels like the breakpoint
# analysis lines aren't 'real'

# My first thought is to color the CQ points based on season, AMC, etc
# this is so overwhleming. 

# I think I'm going to pause here in the analysis, and carry on with the
# other constituents. Once I have those up to this point I can check in with Chuck and Steve






















#### Grouping CQ curves using top 50 ####

# determine the median flow rate for each sites CQ observations: to do this:

df.TP_CQ.top50<-split(df.TP_CQ, f = df.TP_CQ$Name)%>% # split the df into list of dfs for each site, 
  lapply(., \(i) i%>%filter(X_00060_00003>median(i$X_00060_00003)))%>% # filter to the median flow rate for each site,
  bind_rows(.) # bind back into single dataframe:

# now run through grouping CQ workflow with this truncated df:

l.TP_CQ<-df.TP_CQ.top50%>%split(., .$Name) # resplit the df into list:
l.TP_CQ.lm<-lapply(l.TP_CQ, \(i) lm(log_C~log_Q, data=i)) # create lm models for each site:
TP_CQ.coef<-as.data.frame(bind_rows(lapply(l.TP_CQ.lm, \(i) summary(i)$coefficients[,1]), .id = 'site_no'))%>%rename(Intercept = 2, Slope = 3) # save the model coef ad pvals:
TP_CQ.pvals<-as.data.frame(bind_rows(lapply(l.TP_CQ.lm, \(i) summary(i)$coefficients[,4]), .id = 'site_no'))%>%rename(Intercept.pval = 2, Slope.pval = 3) # save the pvalues 
df.lm.top50<-left_join(TP_CQ.coef,TP_CQ.pvals,by='site_no')%>%left_join(., df.TP_CQ%>%select(Name, n_sample_rank)%>%distinct(., .keep_all = T), by = c('site_no' = 'Name')) # merge the two dfs:
df.lm.top50<-mutate(df.lm.top50, Type_top50 = ifelse(Slope.pval>0.05, 'Stationary', ifelse(Slope>0, 'Mobilization', 'Dilution'))) # add column for CQ type:
df.TP_CQ.top50<-left_join(df.TP_CQ.top50,df.lm.top50%>%select(site_no, Type_top50),by=c('Name'='site_no')) # merge CQ type labels to the df for plotting

# here is where it changes: add another column for the y coordinates of the OLS line for just the top 50 percentile:

temp<-lapply(seq_along(l.TP_CQ.lm), \(i) data.frame(Seg_C_log_top50 = fitted(l.TP_CQ.lm[[i]]), l.TP_CQ[[i]]$sample_dt))%>%
  purrr::set_names(names(l.TP_CQ.lm))%>%
  bind_rows(., .id = 'Name')%>%
  rename(site = Name, Date = 3)%>%
  mutate(Date = as.Date(Date))

# now add upper 50 CQ type and Seg_C_log_top50 columns to df_Seg.2:

df_Seg.2<-left_join(df_Seg.2, df.lm.top50%>%select(site_no, Type_top50), by = c('site'='site_no'))%>%
  mutate(Date = as.Date(Date))%>%
  left_join(., temp, by = c('site', 'Date'))

# make plot:

p<-ggplot(df_Seg.2, aes(x = log(Q_real), y = log(C)))+
  geom_point()+
  geom_smooth(aes(color = Type),method = 'lm')+
  scale_color_manual(name = "Full CQ Type", values = c("red", "blue", "green"))+
  ylab('log(TP)')+
  new_scale_color() +
  geom_line(aes(x = log(Q_real), y = Seg_C_log_top50), size = 2.5, color = 'black')+
  geom_line(aes(x = log(Q_real), y = Seg_C_log_top50, color = Type_top50), size = 2)+
  scale_color_manual(name = "Top 50 CQ Type", values = c("blue", "green"))+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'free')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )

# p

# export df_Seg.2 for use in FingerLakesPresentation.R

# save(df_Seg.2, file = 'Processed_Data/TP.df_Seg.2.w_top50.Rdata')

#
























#### Triangle plot ####

# set up dataframe:

# add percent ag and urban to df.lm:

df.tri<-left_join(df.lm, df.NLCD06%>%select(Name, DEVNLCD06, PLANTNLCD06), by = c('site_no'='Name'))%>%select(site_no, Slope, Type, DEVNLCD06, PLANTNLCD06)%>%mutate(Slope = round(Slope, 2), DEVNLCD06=round(DEVNLCD06, 2), PLANTNLCD06=round(PLANTNLCD06, 2))

# create plot:

p<-ggplot(df.tri, aes(x=DEVNLCD06, y=PLANTNLCD06)) +
  geom_point(aes(shape=as.factor(Type), fill=abs(Slope)), size = 4) +
  scale_shape_manual(values = c("Mobilization" = 24, "Dilution" = 25, "Stationary" = 22),
                     guide = guide_legend(override.aes = list(fill = "pink")))+
  scale_fill_gradient(low = "yellow", high = "red")+
  xlab('Percent Developed')+
  ylab('Percent Agriculture')+
  ggtitle(paste(ncode, 'CQ type and slope magnitude as a function of percent Ag and Developed Land'))

p$labels$fill <- "Slope Magnitude"
p$labels$shape <- "CQ Type"

p

# want to recreate this plot in Code/FingerLakesPresentation.R so exporting df.tri:

# save(df.tri, file = 'Processed_Data/df.tri.Rdata')

#

#### Triangle plot with top 50 ####

df.tri<-left_join(df.lm.top50, df.NLCD06%>%select(Name, DEVNLCD06, PLANTNLCD06), by = c('site_no'='Name'))%>%select(site_no, Slope, Type_top50, DEVNLCD06, PLANTNLCD06)%>%mutate(Slope = round(Slope, 2), DEVNLCD06=round(DEVNLCD06, 2), PLANTNLCD06=round(PLANTNLCD06, 2))

# create plot:

p<-ggplot(df.tri, aes(x=DEVNLCD06, y=PLANTNLCD06)) +
  geom_point(aes(shape=as.factor(Type_top50), fill=abs(Slope)), size = 4) +
  scale_shape_manual(values = c("Mobilization" = 24, "Dilution" = 25, "Stationary" = 22),
                     guide = guide_legend(override.aes = list(fill = "pink")))+
  scale_fill_gradient(low = "yellow", high = "red")+
  xlab('Percent Developed')+
  ylab('Percent Agriculture')+
  ggtitle(paste(ncode, 'CQ type and slope magnitude as a function of percent Ag and Developed Land'))

p$labels$fill <- "Slope Magnitude"
p$labels$shape <- "CQ Type"

p

# want to recreate this plot in Code/FingerLakesPresentation.R so exporting df.tri:

# save(df.tri, file = 'Processed_Data/df.tri.Rdata')

#

























#### MLR ####

# create a list of dfs, one for each cQ parameter, with the values of the spearman correlations for each predictor:

l.cor.MLR<-df.cor %>%
  split(., df.cor$CQ_Parameter)%>%
  lapply(., \(i) i%>% 
           arrange(desc(abs(Spearman_Correlation)))%>%  
           # slice_head(n = 7)%>%
           # bind_rows(i%>%arrange(desc(Spearman_Correlation))%>%slice_head(n = 7))%>%
           # distinct(term, .keep_all = T)%>%
           # filter(!between(Spearman_Correlation, -0.25,.25))%>%
           mutate(sig_0.05 = factor(sig_0.05, levels = c('not', 'sig')))%>%
           # mutate(term = factor(term, levels = unique(term[order(Spearman_Correlation)])))%>%
           # filter(sig_0.05 == 'sig')%>%
           as.data.frame())


x<-l.cor.MLR[[1]]

# create a list of dfs from df.OLS.Sens, which contains the values of the predictors, subseting using the names in each l.cor[[i]]$term:

l.cor.MLR.full<-lapply(1:5, \(i) df.OLS_Sens%>%
                         select(i+1, l.cor.MLR[[i]]$term)%>%
                         as.data.frame()%>%
                         rename(term = 1))

names(l.cor.MLR.full)<-names(df.OLS_Sens)[2:6]

y<-l.cor.MLR.full[[1]]

y.round<-round(y, 2)

# create another list but keep the site names for use at end of loop in site outliers:

l.cor.MLR.full.w_name<-lapply(1:5, \(i) df.OLS_Sens%>%
                                select(1, i+1, l.cor.MLR[[i]]$term)%>%
                                as.data.frame())

names(l.cor.MLR.full.w_name)<-names(df.OLS_Sens)[2:6]

z<-l.cor.MLR.full.w_name[[1]]

# Trying no feature selection:

# loop through the 5 CQ parameters:

i<-1

m.list<-list()

for (i in 1:5){
  
  df<-l.cor.MLR.full[[i]]
  
  library(caret)
  
  set.seed(123)
  
  # Set up repeated k-fold cross-validation
  
  train.control <- trainControl(method = "cv", number = 10)
  
  # Train the model
  
  step.model <- train(term ~., data = df,
                      method = "leapForward", 
                      tuneGrid = data.frame(nvmax = 1:5),
                      trControl = train.control
  )
  
  
  # look at model results:
  
  step.model$results
  step.model$bestTune
  summary(step.model$finalModel)
  coef(step.model$finalModel, as.numeric(step.model$bestTune))
  
  # get df of just the predictors in this model:
  
  pred<-names(coef(step.model$finalModel, as.numeric(step.model$bestTune)))[-1]
  
  df.2<-df%>%select(term, pred)
  
  # make lm:
  
  m<-lm(term~., data=df.2)
  
  m.list[[i]]<-m
  
  # summary(m)
  
  # look at univarite plots:
  
  p<-df.2%>%pivot_longer(cols = 2:last_col(), names_to = 'Predictor', values_to = 'Value')%>%
    rename(!!names(l.cor.MLR.full)[i]:=term)%>%
    ggplot(., aes(x=Value, y=!!sym(names(l.cor.MLR.full)[i])))+
    geom_point()+
    geom_smooth(method = 'lm')+
    facet_wrap('Predictor', scales = 'free')
  
  # look at model residuals:
  
  # plot(m)
    
}

# set m.list names:

names(m.list)<-names(l.cor.MLR.full)

tab_model(m.list, dv.labels = names(m.list), title = paste('Comparison of MLR models for',ncode, 'using forward selection implemented in caret::train'), file="temp.html")

# export m.list for use in Code/FingerLakesPresentation.R:

# save(m.list, file = 'Processed_Data/m.list.TP.Rdata')

#



























#### MLR with observations greater than the median ####

# determine the median flow rate for each sites CQ observations: to do this:

df.TP_CQ.top50<-split(df.TP_CQ, f = df.TP_CQ$Name)%>% # split the df into list of dfs for each site, 
  lapply(., \(i) i%>%filter(X_00060_00003>median(i$X_00060_00003)))%>% # filter to the median flow rate for each site,
  bind_rows(.) # bind back into single dataframe:

# now run through the MLR workflow:

l.cor.MLR<-df.cor.top50 %>% # create a list of dfs, one for each cQ parameter, with the values of the spearman correlations for each predictor:
  split(., df.cor.top50$CQ_Parameter)%>%
  lapply(., \(i) i%>% 
           arrange(desc(abs(Spearman_Correlation)))%>%  
           # slice_head(n = 7)%>%
           # bind_rows(i%>%arrange(desc(Spearman_Correlation))%>%slice_head(n = 7))%>%
           # distinct(term, .keep_all = T)%>%
           # filter(!between(Spearman_Correlation, -0.25,.25))%>%
           mutate(sig_0.05 = factor(sig_0.05, levels = c('not', 'sig')))%>%
           # mutate(term = factor(term, levels = unique(term[order(Spearman_Correlation)])))%>%
           # filter(sig_0.05 == 'sig')%>%
           as.data.frame())
l.cor.MLR.full<-lapply(1:5, \(i) df.OLS_Sens.top50%>% # create a list of dfs from df.OLS.Sens, which contains the values of the predictors, subseting using the names in each l.cor[[i]]$term:
                         select(i+1, l.cor.MLR[[i]]$term)%>%
                         as.data.frame()%>%
                         rename(term = 1))
names(l.cor.MLR.full)<-names(df.OLS_Sens.top50)[2:6]
l.cor.MLR.full.w_name<-lapply(1:5, \(i) df.OLS_Sens.top50%>% # create another list but keep the site names for use at end of loop in site outliers:
                                select(1, i+1, l.cor.MLR[[i]]$term)%>%
                                as.data.frame())
names(l.cor.MLR.full.w_name)<-names(df.OLS_Sens.top50)[2:6]
m.list<-list() # create list to store model objects
i<-1
for (i in 1:5){ # loop through the 5 CQ parameters:
  df<-l.cor.MLR.full[[i]]
  library(caret)
  set.seed(123)
  train.control <- trainControl(method = "cv", number = 10) # Set up repeated k-fold cross-validation
  step.model <- train(term ~., data = df, method = "leapForward", tuneGrid = data.frame(nvmax = 1:5),trControl = train.control) # Train the model
  step.model$results  # look at model results:
  step.model$bestTune
  summary(step.model$finalModel)
  coef(step.model$finalModel, as.numeric(step.model$bestTune))
  pred<-names(coef(step.model$finalModel, as.numeric(step.model$bestTune)))[-1] # get df of just the predictors in this model:
  df.2<-df%>%select(term, pred)
  m<-lm(term~., data=df.2) # make lm:
  m.list[[i]]<-m # append model to list
  summary(m)
  p<-df.2%>%pivot_longer(cols = 2:last_col(), names_to = 'Predictor', values_to = 'Value')%>% # look at univarite plots:
    rename(!!names(l.cor.MLR.full)[i]:=term)%>%
    ggplot(., aes(x=Value, y=!!sym(names(l.cor.MLR.full)[i])))+
    geom_point()+
    geom_smooth(method = 'lm')+
    facet_wrap('Predictor', scales = 'free')
  p
  # plot(m)  # look at model residuals:
  
}

# set m.list names:

names(m.list)<-names(l.cor.MLR.full)

tab_model(m.list, dv.labels = names(m.list), title = paste('Comparison of MLR models for',ncode, 'using forward selection implemented in caret::train'), file="temp.top50.html")

#










































#### Checking Dam Storage and Corn ####

# It was found that dam storage was showing up in the MLR model (done later in the code)
# And that corn was shwoing up in the later editions of the MLR with the reduced datasets
# so this seciton it to see if thetwo correlate and also make maps of dam storage, corn, etc.






























#### 













# I would chose the following sites as complex:

complex_sites<-unique(df_Seg.2$site)[c(3,16,21,22,29,37)] 

# old complex site (when n=53): c(3,18,24,25,26,35,42,43)

df_Seg.2<-mutate(df_Seg.2, Type = ifelse(site %in% complex_sites, 'Complex', Type))

# make plot:

p<-ggplot(df_Seg.2, aes(x = log(Q_real), y = log(C)))+
  geom_point(aes(color = Type))+
  scale_color_manual(name = "CQ Type", values = c("purple", "red", "blue", "green"))+
  geom_smooth(method = 'lm')+
  geom_line(aes(x = Q, y = Seg_C), color = 'yellow', size = 1.5)+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'free')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )

p


# plot:

p<-ggplot(df_Seg.2, aes(x = log(Q_real), y = log(C)))+
  geom_point(aes(color = Type), size = 1.5)+
  scale_color_manual(name = "CQ Type", values = c("purple", "red", "blue", "green"))+
  geom_smooth(method = 'lm')+
  geom_line(aes(x = Q, y = Seg_C), color = 'yellow', size = 1.5)+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'free')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )+
  geom_rect(data = df_Seg.2%>%distinct(df_Seg.2$site, .keep_all = T), aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, fill = USGS.LU.Adjusted), alpha = .15)+
  scale_fill_manual(name = "USGS Landuse\n(Adjusted)", values = c("red", "blue","purple", "green"))

p

# plot of trouble sites from MLR analysis:


p<-filter(df_Seg.2, site %in% look_at_sites)%>%
  ggplot(., aes(x = log(Q_real), y = log(C)))+
  geom_point(aes(color = Type), size = 1.5)+
  scale_color_manual(name = "CQ Type", values = c("purple", "red", "blue", "green"))+
  geom_smooth(method = 'lm')+
  geom_line(aes(x = Q, y = Seg_C), color = 'yellow', size = 1.5)+
  # geom_text(aes(x = 2, y = -2, label = CAFO_count), inherit.aes = FALSE, size = 30)+
  facet_wrap(dplyr::vars(site), scales = 'free')+
  # theme(
  #   strip.background = element_blank(),
  #   strip.text.x = element_blank()
  # )+
  geom_rect(data = .%>%distinct(.$site, .keep_all = T), aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, fill = USGS.LU.Adjusted), alpha = .15)+
  scale_fill_manual(name = "USGS Landuse\n(Adjusted)", values = c("red", "blue","purple", "green"))

p

# save(p1, file = 'Processed_Data/p1.Rdata') 

# add CQ type to the mapping df with polygon trasperncy based on number of samples:

map.CQ_Type<-df.sf.NWIS.keep%>%
  filter(Name %in% df.datalayers$Name)%>%
  left_join(.,distinct(df_Seg.2, site, .keep_all = T)%>%select(.,c(site, Type, n_sample_rank)),  by = c('Name'='site'))%>%
  select(Name, Type, n_sample_rank)%>%
  arrange(n_sample_rank)%>%
  mutate(NEW = (1/row_number())*40)
# left_join(.,df.NWIS.TP%>%group_by(site_no)%>%summarise(n=n()), by = c('site'='site_no'))%>%
# mutate(NEW = case_when(n < 50 ~ .05,
#                        n >=50 & n < 100 ~ .25,
#                        n >=100 & n < 500 ~ .5,
#                        n >=500 ~ .9))
# map:

mapview(map.CQ_Type, zcol = 'Type', alpha.regions = 'NEW')

# map of trouble sites in MLR section:

x<-map.CQ_Type%>%filter(Name %in% look_at_sites)

mapview(x, zcol = 'Type')

######################################################
#### ~~~~ XY plot of Slopes and Intercepts ~~~~ ####
####################################################

# plot of slopes and intercepts for each CQ type:

m<-m%>%mutate(Type = ifelse(site_no %in% complex_sites, 'Complex', Type))%>%
  left_join(., df.datalayers%>%select(Name, USGS.LU.Adjusted), by = c('site_no'='Name'))

ggplot(m, aes(x=`(Intercept)`, y=log_Q, color = Type))+
  geom_point(size = 2)+
  scale_color_manual(name = "CQ Type", values = c("purple", "red", "blue", "green"))+
  facet_wrap('USGS.LU.Adjusted', scales = 'fixed')

# I want to do this with more sites























#### Land Use changes across watersheds (not run) ####

# # Look at changes in NLCD and CDL calculated Ag %land between 2008-2020 for each 53 watershed
# # to do this:
# 
# # pivot longerto getthe land use catgories in a single column:
# 
# df.LU.2<-pivot_longer(df.LU, cols = 6:12, names_to = 'LandUse', values_to = 'values')
# 
# # pivot wider to get the land use source (data layers) in their own columns:
# 
# df.LU.2<-pivot_wider(df.LU.2, names_from = 'LU_source', values_from = 'values')
# 
# # pivot longer to put CDL and NLCD in own columns:
# 
# df.LU.CDL<-pivot_longer(df.LU.2, cols = starts_with('CDL'), names_to = 'CDL', values_to = 'CDL_values')
# df.LU.NLCD<-pivot_longer(df.LU.2, cols = starts_with('NLCD'), names_to = 'NLCD', values_to = 'NLCD_values')
# 
# # group by land use and calculate the difference between the years:
# 
# df.LU.2<-df.LU.2%>%mutate(CDL_diff = abs(round(CDL_2020-CDL_2008,2)),NLCD_diff = abs(round(NLCD_2019-NLCD_2001,2)))%>%
#   arrange(desc(CDL_diff))
# 
























#### Seasonal ANanlysis ####

# is the CQ relaitonship different between seasons at these sites?

# I need to filter down to sites with over 100 samples for this, 20 samples isnt going tocut it:

df_Seg.3<-filter(df_Seg.2, n>100)

# add season column:

df_Seg.3$Season<-getSeason(df_Seg.3$Date)

# plot:

p1<-ggplot(df_Seg.3, aes(x = log(Q_real), y = log(C)))+
  geom_point(aes(color = Season), size = 4)+
  # scale_color_manual(name = "CQ Type", values = c("purple", "red", "blue", "green"))+
  geom_smooth(method = 'lm')+
  geom_line(aes(x = Q, y = Seg_C), color = 'yellow', size = 1.5)+
  # geom_text(aes(x = 2, y = -2, label = CAFO_count), inherit.aes = FALSE, size = 30)+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'free')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank(),
    legend.position = "bottom",
    legend.title=element_text(size=14)
  )+
  geom_rect(data = df_Seg.3%>%distinct(df_Seg.3$site, .keep_all = T), aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, fill = Type), alpha = .15)+
  scale_fill_manual(name = "CQ Type", values = c("red", "blue","purple", "green"))

p1

# save(p1, file = 'Processed_Data/p1.Rdata') 

# 


























############################################
#### ~~~~ Q Exceedence Proability ~~~~ ####
############################################




























####~~~~ Calcualte EP ~~~~####

# subset the daily flows to the 40 sites:

temp<-filter(df.NWIS.Q, site_no %in% df.datalayers$Name)

# split into lists by site:

temp<-split(temp, f = temp$site_no) 

# sort each dataframe in the list by the flow, add a column for m and n, convert back to a dataframe using bind_rows, and select only the needed columns for the left join (next step):

temp<-bind_rows(lapply(temp, \(i) i[order(i$X_00060_00003,decreasing = T),]%>%mutate(m = 1:n(), n = n())))%>%select(site_no, Date, m, n)

# append the value of M and n for each C-Q observation in the C-Q dataframe:
# need to rename the n column in the left dataframe as well:

df_Seg.2<-left_join(df_Seg.2%>%rename(n_samples = n), temp, by = c("site" = "site_no", "Date" = "Date"))

# calcualte the exceednce proability for Q for each C observation:

df_Seg.2$EP<-round(df_Seg.2$m/(df_Seg.2$n+1), 4)

# plot:

p1<-ggplot(df_Seg.2, aes(x = EP, y = log(C)))+
  geom_point(aes(color = Type))+
  scale_color_manual(name = "Log-Log\nCQ Type", values = c("purple", "red", "blue", "green"))+
  geom_smooth(method = 'lm')+
  # geom_line(aes(x = Q, y = Seg_C), color = 'yellow', size = 1.5)+
  # geom_text(aes(x = .5, y =-2, label = CAFO_count), inherit.aes = FALSE, size=15)+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'free')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )+
  scale_x_reverse()+
  geom_rect(data = df_Seg.2%>%distinct(df_Seg.2$site, .keep_all = T), aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, fill = USGS.LU.Adjusted), alpha = .15)+
  scale_fill_manual(name = "USGS Landuse\n(Adjusted)", values = c("red", "blue","purple", "green"))

p1

# save(p1, file = 'Processed_Data/p1.Rdata')


























####~~~~ Estimate Break point analysis using EP-Q ~~~~####

#  rerun the BP analysis using EP-Q:

l_Seg.EP<-list()
davies.test.matrix.EP<-NULL
EP<-df.NWIS.TP_CQ%>%
  filter(site_no %in% df.datalayers$Name)%>%
  left_join(., df_Seg.2%>%select(site, Date, EP), by = c('site_no'='site', 'sample_dt'='Date'))%>%
  drop_na(result_va, Q_yield)%>%
  rename(site=site_no, Date = sample_dt)
temp.loop<-sort(unique(EP$site))

i<-1

for (i in seq_along(temp.loop)){
  
  tryCatch({
    
    # print the site name for loop debugging:
    
    print(i)
    print(temp.loop[i])
    
    # create a dataframe that will work with segmented. To do this: 
    # filter for the site the loop is in
    # add log transformed C and Q columns, as well as duplicated columns for renamed C and Q
    # filter for real log C and Q values so breakpoint analysis works smoothly:
    
    df<-EP%>%
      filter(site == temp.loop[i])%>%
      mutate(C = result_va)%>%
      mutate(log_C = log(C))%>%
      filter(is.finite(log_C))%>%
      filter(is.finite(EP))
    
    # build a single slope lm for log C and Q. Tis model is also used inthe breakpoint analysis inthenext step:
    
    m<-lm(log_C~EP, df)
    
    # perform breakpoint regression:
    
    m_seg<-segmented(obj = m, npsi = 1)
    
    # perform davies test for constant linear predictor:
    # the results are saved as a string with the the site name and true/false:
    
    x<-paste(temp.loop[i], '-', davies.test(m)$p.val<0.05)
    
    # add the results of davies test to the matrix made prior to this for loop:
    
    davies.test.matrix.EP<-c(davies.test.matrix.EP,x)
    
    # save the breakpoints
    
    bp<-m_seg$psi[1]
    
    # save the slopes: To do this:
    # a conditional statement is needed since sometimes the segmented function wont fit a two slope model at all and will return a object that doesnt work with the slope function used here:
    
    if(length(class(m_seg))==2){
      s<-as.data.frame(slope(m_seg))
    } else{
      s<-NA
    }
    
    # get the intercepts (again conditional statement is needed):
    
    if(length(class(m_seg))==2){
      inter<-as.data.frame(intercept(m_seg))
    } else{
      inter<-NA
    }
    
    
    # get the model fitted data, put in a dataframe, and reformat this dataframe to export out of the loop
    
    if(length(class(m_seg))==2){
      fit <- data.frame(site = df$site, Date = df$Date, Seg_C_EP = fitted(m_seg), I1_EP = inter$Est.[1], I2_EP = inter$Est.[2], Slope1_EP = s[1,1], Slope2_EP = s[2,1], BP_EP = bp)
      result_df<-left_join(df, fit, by = c('site', 'Date'))
    } else{
      fit <- data.frame(site = df$site, Date = df$Date, Seg_C_EP = fitted(m_seg), I1_EP = inter$Est.[1], I2_EP = NA, Slope1_EP = NA, Slope2_EP = NA, BP_EP = NA)
      result_df<-left_join(df, fit, by = c('site', 'Date'))
    }
    
    l_Seg.EP[[i]]<-result_df
    
  }, 
  
  error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
  
}

davies.test.matrix.EP
df.davies.EP<-as.data.frame(davies.test.matrix.EP)%>%
  separate_wider_delim(1, "-", names = c("site", "BP_EP_yes"))%>%
  mutate(across(c(1,2), trimws))
df_Seg.EP<-bind_rows(l_Seg.EP)%>%
  mutate(Seg_EP=EP)
df_Seg.EP<-df_Seg.EP%>%
  left_join(., df.davies.EP, by = 'site')%>%
  mutate(across(c(Seg_C_EP, Seg_EP), ~replace(., BP_EP_yes == 'FALSE', NA)))

# merge the land use, CQ type, data to this df:

df_Seg.EP<-left_join(df_Seg.EP, m%>%select(site_no, Type, USGS.LU.Adjusted), by = c('site'='site_no'))

# plot:

p<-ggplot(df_Seg.EP, aes(x = EP, y = log(C)))+
  geom_point(aes(color = Type), size = 1.5)+
  scale_color_manual(name = "Log-log\nCQ Type", values = c("purple", "red", "blue", "green"))+
  geom_smooth(method = 'lm')+
  new_scale_color()+
  geom_line(aes(x = Seg_EP, y = Seg_C_EP), color = 'yellow', size = 1.5)+
  scale_color_manual(name = "Log(C)~EP-Q\n Breakpoint Analysis", values = "yellow")+
  # geom_text(aes(x = 2, y = -2, label = CAFO_count), inherit.aes = FALSE, size = 30)+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'free_y')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )+
  scale_x_reverse()+
  geom_rect(data = df_Seg.EP%>%distinct(df_Seg.EP$site, .keep_all = T), aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, fill = USGS.LU.Adjusted), alpha = .15)+
  scale_fill_manual(name = "USGS Landuse\n(Adjusted)", values = c("red", "blue","purple", "green"))

p

save(p, file = 'Processed_Data/p1.Rdata')

# I like the idea of using the EP plots to help ID which
# sites are complex. For example, site 22 was deemed complex
# with log-log plots, but in log-EP space it looks similar to
# the other mobilization sites. 























#----------------------------------------#
####~~~  Reclassifying CQ type ~~~#### 
###~~~  using the EP-Q plots  ~~~### 
#----------------------------------------#

# rerun the df_Seg.2 workflow bt=ut changing the complex_sites vector:
temp<-df.NWIS.TP_CQ%>%
  rename(Name = site_no)%>%
  filter(Name %in% df.datalayers$Name)%>%
  mutate(log_C = log(result_va), log_Q = log(X_00060_00003), C = result_va, Q = X_00060_00003)%>%
  filter(is.finite(log_C))%>%
  filter(is.finite(log_Q))
l.temp<-temp%>%
  split(., .$Name)
l.lm.CQ_slopes<-lapply(l.temp, \(i) lm(log_C~log_Q, data=i))
coef<-tibble::rownames_to_column(as.data.frame(t(sapply(l.lm.CQ_slopes, \(i) summary(i)$coefficients[,1] ))), 'site_no')
pvals<-tibble::rownames_to_column(as.data.frame(t(sapply(l.lm.CQ_slopes, \(i) summary(i)$coefficients[,4] ))), 'site_no')%>%
  rename(I.pval = 2, S.pval = 3)
m<-left_join(coef,pvals,by='site_no')
m<-mutate(m, Type = ifelse(S.pval>0.05, 'Stationary', ifelse(log_Q>0, 'Mobilization', 'Dilution')))
temp<-left_join(temp,m%>%select(site_no, Type),by=c('Name'='site_no'))
df_Seg.2<-filter(df_Seg, site %in% temp$Name)%>%
  left_join(.,m%>%select(site_no, Type),by=c('site'='site_no')) 
df_Seg.2<-df_Seg.2%>%
  mutate(slope_angle=factor(round(atan(abs((Slope2-Slope1)/(1+(Slope2*Slope1)))),1)))
hc<-heat.colors(length(unique(df_Seg.2$slope_angle)), rev = T)
complex_sites<-unique(df_Seg.2$site)[c(3,16,21,37)] 
df_Seg.2<-mutate(df_Seg.2, Type = ifelse(site %in% complex_sites, 'Complex', Type))
df_Seg.2<-left_join(df_Seg.2, df.datalayers%>%select(Name, USGS.LU.Adjusted, CAFO_count), by = c('site'='Name'))
df_Seg.2$CAFO_count[df_Seg.2$CAFO_count==0]<-NA

# plot:

p<-ggplot(df_Seg.2, aes(x = log(Q_real), y = log(C)))+
  geom_point(aes(color = Type), size = 1.5)+
  scale_color_manual(name = "CQ Type", values = c("purple", "red", "blue", "green"))+
  geom_smooth(method = 'lm')+
  geom_line(aes(x = Q, y = Seg_C), color = 'yellow', size = 1.5)+
  # geom_text(aes(x = 2, y = -2, label = CAFO_count), inherit.aes = FALSE, size = 30)+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'free')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )+
  geom_rect(data = df_Seg.2%>%distinct(df_Seg.2$site, .keep_all = T), aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, fill = USGS.LU.Adjusted), alpha = .15)+
  scale_fill_manual(name = "USGS Landuse\n(Adjusted)", values = c("red", "blue","purple", "green"))

p

#







































































#### Delinating Watersheds (not run) ####

# # delinate the 137 NWIS sites:
# 
# l.SS_WS.NWIS<-lapply(seq_along(df.NWIS.TP_site_metadata$site_no), \(i) Delineate(df.NWIS.TP_site_metadata$dec_long_va[i], df.NWIS.TP_site_metadata$dec_lat_va[i]))
# 
# names(l.SS_WS.NWIS)<-df.NWIS.TP_site_metadata$site_no
# 
# # save(l.SS_WS.NWIS, file = 'C:/PhD/CQ/Downloaded_Data/l.SS_WS.NWIS.Rdata')
# 
# df.sf.NWIS<-fun.l.SS_WS.to.sfdf(l.SS_WS.NWIS)
# 
# # save(df.sf.NWIS, file = 'C:/PhD/CQ/Processed_Data/df.sf.NWIS.Rdata')
# 
# load('C:/PhD/CQ/Processed_Data/df.sf.NWIS.Rdata')
# 
# # calculate DA using vect:
# 
# vect.NWIS<-vect(df.sf.NWIS)
# 
# vect.NWIS$area_KM2<-expanse(vect.NWIS, unit="km")
# 
# # add DA in mi2 to df.sf:
# 
# df.sf.NWIS$area_sqmi<-expanse(vect.NWIS, unit="km")*0.386102
# 
# # add NWIS tabulated area to df sf: first download the metadata for the sites using readNWISsite, then merge drain_area_va column to sfdf:
# 
# temp<-readNWISsite(df.sf.NWIS$Name)
# 
# df.sf.NWIS<-left_join(df.sf.NWIS, temp[,c(2,30)], by = c('Name'='site_no'))
# 
# # calcuate the percent error of the delination:
# 
# df.sf.NWIS$Delination_Error<-(df.sf.NWIS$area_sqmi-df.sf.NWIS$drain_area_va)/df.sf.NWIS$drain_area_va
# 
# # I want to look at the delimaitons that are on the cusp of not working:
# 
# temp<-df.sf.NWIS[,c(1,107)]%>%
#   filter(abs(Delination_Error)>.02)
# 
# # mapview(temp)
# 
# # after playing with the numbers, 2% seems like a good threshold for non-delineations:
# # subset df.sf to those sites that are considered to delineate correctly:
# 
# df.sf.NWIS.keep<-df.sf.NWIS%>%filter(abs(Delination_Error)<=.02)
# 
# # note I suspect that snapping the lat long to nhd would improve the delineation success notgoing to do that now









































############################################################
########~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~########
####~~~~~~~~~~~~~~~~~Watershed Attributes (NOT RUN)~~~~~~~~~~~~~~~####
####~~~~~~~~~~~~(for correlation, eventually)~~~~~~~~~~~####
########~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~########
############################################################



####~~~~ Data Layers ~~~~####

#### CDL: ####

# note when I ran ths CDL code block I used the 103 sites thatwere apartofthe set, this is prior to filtering based onthe CDLdate... so I am just going to filter the resulting CDL df at the end. Just note that what is saved is of the 103 sites, but that the code doesnt reflect this, i.e. I just ran the filter anddidnt keep the code because df.NWIS.TP.keep would be down to 56 if I did it right 

# download:

rast.NWIS.CDL.2020 <- GetCDLData(aoi = df.sf.NWIS, year = "2020", type = "b", tol_time = 1000)
rast.NWIS.CDL.2008 <- GetCDLData(aoi = df.sf.NWIS, year = "2008", type = "b", tol_time = 1000)

# convert to rast:

rast.NWIS.CDL.2020<-rast(rast.NWIS.CDL.2020)
rast.NWIS.CDL.2008<-rast(rast.NWIS.CDL.2008)

# check CRS of both years:

crs(rast.NWIS.CDL.2020)
crs(rast.NWIS.CDL.2008)

# plot

# plot(rast.NWIS.CDL.2020)
# plot(rast.NWIS.CDL.2008)

# reproject to sample watershed vector data to match raster data:

vect.NWIS<-vect(df.sf.NWIS) # need to first ininalize vect since sometimes reading in rdata file (terra issue)

vect.NWIS.proj<-terra::project(vect.NWIS, crs(rast.NWIS.CDL.2008))

# extract frequency tables for each sample watershed

l.NWIS.CDL <- terra::extract(rast.NWIS.CDL.2020, vect.NWIS.proj, table,ID=FALSE)[[1]]
l.NWIS.CDL.2008 <- terra::extract(rast.NWIS.CDL.2008, vect.NWIS.proj, table,ID=FALSE)[[1]]

# save:

# save(l.NWIS.CDL, file='Processed_Data/l.NWIS.CDL.Rdata')
# save(l.NWIS.CDL.2008, file='Processed_Data/l.NWIS.CDL.2008.Rdata')

load('Processed_Data/l.NWIS.CDL.Rdata')
load('Processed_Data/l.NWIS.CDL.2008.Rdata')

# convert resulting list of tables to list of dfs

l.NWIS.CDL<-lapply(l.NWIS.CDL, as.data.frame)
l.NWIS.CDL.2008<-lapply(l.NWIS.CDL.2008, as.data.frame)

# the next step is to join the CDL key with these dfs, but first:

# aggregate CDL: to do this:
# looking at the CDL legend (linkdata), I determined the following:

Ag<-c(1:6,10:14,21:39,41:61,66:72,74:77,204:214,216:227,229:250,254)
Pasture<-c(176)
Forest<-c(63,141:143)
Developed<-c(82,121:124)
Water<-c(83,111)
Wetlands_all<-c(87,190,195)
Other<-c(64,65,88,112,131,152) # Shrub<-c(64,152) Barren<-c()

l <- tibble::lst(Ag,Pasture,Forest,Developed,Water,Wetlands_all,Other)

reclass_CDL<-data.frame(lapply(l, `length<-`, max(lengths(l))))%>%
  pivot_longer(cols = everything(), values_to = 'MasterCat',names_to = 'Crop')%>%
  drop_na(MasterCat)

# left join each df in the list to the CDL legend key, as well as calcuate the pland:
  
l.NWIS.CDL<-lapply(l.NWIS.CDL, \(i) i%>%mutate(Var1 = as.integer(as.character(Var1)),Freq=round(Freq/sum(Freq),2))%>%dplyr::left_join(., reclass_CDL, by = c('Var1' = 'MasterCat'))) # I replaced linkdata in the left join with reclass_CDL to get simplified CDL classes
l.NWIS.CDL.2008<-lapply(l.NWIS.CDL.2008, \(i) i%>%mutate(Var1 = as.integer(as.character(Var1)),Freq=round(Freq/sum(Freq),2))%>%dplyr::left_join(., reclass_CDL, by = c('Var1' = 'MasterCat'))) # I replaced linkdata in the left join with reclass_CDL to get simplified CDL classes

# set names of list:

names(l.NWIS.CDL)<-df.sf.NWIS$Name # note doesnt work with the workflow set up to filter on 2008 limiter
names(l.NWIS.CDL.2008)<-df.sf.NWIS$Name # note doesnt work with the workflow set up to filter on 2008 limiter

# combine list into single df:

df.NWIS.CDL<-bind_rows(l.NWIS.CDL, .id = 'Name')
df.NWIS.CDL.2008<-bind_rows(l.NWIS.CDL.2008, .id = 'Name')

# remove potential for one of the MasterCats in the CDL to be empty, which is messing with the pivot_wider below:

df.NWIS.CDL<-filter(df.NWIS.CDL, Crop != '')
df.NWIS.CDL.2008<-filter(df.NWIS.CDL.2008, Crop != '')

# pivot wider:

# if using the CDL linkdata, use this:

# df.NWIS.CDL<- pivot_wider(df.NWIS.CDL[,-2], names_from = Crop, values_from = Freq)
# df.NWIS.CDL.2008<- pivot_wider(df.NWIS.CDL.2008[,-2], names_from = Crop, values_from = Freq)

# if using the reclass_CDL data, use this:

df.NWIS.CDL<-df.NWIS.CDL[,-2]%>%
  group_by(Name, Crop)%>%
  summarise(Freq=sum(Freq, na.rm = T))%>%
  pivot_wider(., names_from = Crop, values_from = Freq)

df.NWIS.CDL.2008<-df.NWIS.CDL.2008[,-2]%>%
  group_by(Name, Crop)%>%
  summarise(Freq=sum(Freq, na.rm = T))%>%
  pivot_wider(., names_from = Crop, values_from = Freq)

# check to see if add up to 100%:

sort(rowSums(df.NWIS.CDL[,-1], na.rm = T))
sort(rowSums(df.NWIS.CDL.2008[,-1], na.rm = T))

# one of the CDL 2020 is low:

which.min(rowSums(df.NWIS.CDL[,-1], na.rm = T))

# this is a long island site, so doesnt matter

# other than that site looks good. 

# Done with CDL

# actually I want to come up with a workflow to get all years of CDL for the watersheds,
# butthat isgoing totake a long timeto run...

# but first I'm going to look at the differences in the CDL between 2008 and 2020


























#### NLCD ####

# the CDL and NLCD differ in that the CDL combined Pasture and grassland while the nLCD does not
# thus, even though the CDL can be agrgated to et close to the NLCD, I want to also have the nLCD
# to the difference.

# note: the 53 sites post filtering based onthe CDLdate are used in this workflow

# I am going to run this workflow for NLCD 2019 and NLCD 2001 to see if any sites had major changes in land use:

# download: to do this:
# NLCD is not working when trying to download based on the entire polygon df, so going to use lapply to download individually:

l.rast.NWIS.NLCD.2019 <- lapply(seq_along(df.sf.NWIS.keep$Name), \(i) get_nlcd(template = st_cast(df.sf.NWIS.keep, "MULTIPOLYGON")[i,], label = as.character(i), year = 2019))
l.rast.NWIS.NLCD.2001 <- lapply(seq_along(df.sf.NWIS.keep$Name), \(i) get_nlcd(template = st_cast(df.sf.NWIS.keep, "MULTIPOLYGON")[i,], label = as.character(i), year = 2001))

# save(l.rast.NWIS.NLCD.2019, file='Downloaded_Data/l.rast.NWIS.NLCD.2019.Rdata')
# save(l.rast.NWIS.NLCD.2001, file='Downloaded_Data/l.rast.NWIS.NLCD.2001.Rdata')

# convert to SpatRasters:

l.rast.NWIS.NLCD.2019<-lapply(l.rast.NWIS.NLCD.2019, rast)
l.rast.NWIS.NLCD.2001<-lapply(l.rast.NWIS.NLCD.2001, rast)

# plot

# plot(rast.NWIS.NLCD.2019)

# see if 2001 and 2019 crs are the same:

crs(l.rast.NWIS.NLCD.2019[[2]])
crs(l.rast.NWIS.NLCD.2001[[2]])

# reproject to sample watershed vector data to match raster data:

vect.NWIS<-vect(df.sf.NWIS.keep) # need to first ininalize vect since sometimes reading in rdata file (terra issue)

vect.NWIS.proj<-terra::project(vect.NWIS, crs(l.rast.NWIS.NLCD.2019[[1]]))

# extract frequency tables for each sample watershed

system.time({l.NWIS.NLCD.2019 <- lapply(seq_along(l.rast.NWIS.NLCD.2019), \(i) terra::extract(l.rast.NWIS.NLCD.2019[[i]], vect.NWIS.proj[i], ID=FALSE)%>%group_by_at(1)%>%summarize(Freq=round(n()/nrow(.),2)))})
system.time({l.NWIS.NLCD.2001 <- lapply(seq_along(l.rast.NWIS.NLCD.2001), \(i) terra::extract(l.rast.NWIS.NLCD.2001[[i]], vect.NWIS.proj[i], ID=FALSE)%>%group_by_at(1)%>%summarize(Freq=round(n()/nrow(.),2)))})

# save(l.NWIS.NLCD.2019, file = 'Processed_Data/l.NWIS.NLCD.2019.Rdata')
# save(l.NWIS.NLCD.2001, file = 'Processed_Data/l.NWIS.NLCD.2001.Rdata')

load("Processed_Data/l.NWIS.NLCD.2019.Rdata")
load("Processed_Data/l.NWIS.NLCD.2001.Rdata")

# reclassify: to do this:

# adjust the NLCD reclassify legend to match the CDL reclassify df made above:

legend.NWIS<-legend; legend.NWIS$Class3[c(13,17)]<-'Pasture';legend.NWIS$Class3[14]<-'Other';legend.NWIS$Class3[1]<-'Water';legend.NWIS$Class3[c(19,20)]<-'Wetlands_all'

sort(unique(legend.NWIS$Class3))==sort(unique(reclass_CDL$Crop))

# looks good

# reclassify the NLCD using this new legend and clean up the dataframe from the next step

l.NWIS.NLCD.2019<-lapply(l.NWIS.NLCD.2019, \(i) left_join(as.data.frame(i), legend.NWIS%>%select(Class, Class3), by = 'Class')%>%mutate(Class = Class3)%>%select(-Class3))
l.NWIS.NLCD.2001<-lapply(l.NWIS.NLCD.2001, \(i) left_join(as.data.frame(i), legend.NWIS%>%select(Class, Class3), by = 'Class')%>%mutate(Class = Class3)%>%select(-Class3))

# pivot_wider the df in the lists and add a Name column for the site:

l.NWIS.NLCD.2019<-lapply(seq_along(l.NWIS.NLCD.2019), \(i) l.NWIS.NLCD.2019[[i]]%>%group_by(Class)%>%summarise(Freq = sum(Freq))%>%pivot_wider(names_from = Class, values_from = Freq)%>%mutate(Name = df.sf.NWIS.keep$Name[i], .before = 1)%>%as.data.frame(.))
l.NWIS.NLCD.2001<-lapply(seq_along(l.NWIS.NLCD.2001), \(i) l.NWIS.NLCD.2001[[i]]%>%group_by(Class)%>%summarise(Freq = sum(Freq))%>%pivot_wider(names_from = Class, values_from = Freq)%>%mutate(Name = df.sf.NWIS.keep$Name[i], .before = 1)%>%as.data.frame(.))

# bind the lists into a single dataframe
# note some of the sites have different length dtaframes because they didnt have all the same number of NLCD classes. When binding rows this will give a dataframe of the maximum length and put NAs for sites where there wasn't a column: 

df.NWIS.NLCD.2019<-bind_rows(l.NWIS.NLCD.2019)
df.NWIS.NLCD.2001<-bind_rows(l.NWIS.NLCD.2001)

# create a list of these two dataframes:

l.NWIS.NLCD<-list(df.NWIS.NLCD.2019,df.NWIS.NLCD.2001)%>%purrr::set_names(c('2019','2001'))

# save thisprocessed list:

# save(l.NWIS.NLCD, file='Processed_Data/l.NWIS.NLCD.Rdata')

load('Processed_Data/l.NWIS.NLCD.Rdata')

#

























#### NED ####

# download:

DEM.NWIS<-get_ned(df.sf.NWIS, label = '2') # already SpatRaster!

writeRaster(DEM.NWIS, file='Downloaded_Data/DEM.NWIS.tif', overwrite=TRUE)

# plot(DEM.NWIS)

# extract elevation metrics over each sample watershed: to do this:
# build a function with multiple functions:

f <- function(x, na.rm = T) {
  c(mean=mean(x, na.rm = na.rm),
    range=max(x, na.rm = na.rm)-min(x, na.rm = na.rm),
    sd=sd(x, na.rm = na.rm)
  )
}

# reproject NWIS basins to DEM crs:

vect.NWIS<-vect(df.sf.NWIS)

vect.NWIS.proj<-terra::project(vect.NWIS, crs(DEM.NWIS))

# extract the metrics over each watershed using the function above:

df.NWIS.DEM <- as.data.frame(terra::extract(DEM.NWIS, vect.NWIS.proj, f))

# set the names of the df:

names(df.NWIS.DEM)<-c('Name', 'Elev_Avg', 'Elev_Range', 'Elev_SD')

# set the names of the sites:

df.NWIS.DEM$Name<-df.sf.NWIS$Name

# finally save the df:

# save(df.NWIS.DEM, file= 'Processed_Data/df.NWIS.DEM.Rdata')

load('Processed_Data/df.NWIS.DEM.Rdata')

# done with DEM

























#### Climate (not run) ####

# steps:

# Loop through the three metrics and metric specific operations.
# for each metric, perform:

# 1) download all gridded climate data for a metric (e.g.: precip)
# 2) extract daily mean of the metric for each subbasin into df
# 3) reformat (pivot longer),convert the date, and calcualte annual statistics based on the metric (e.g. precip is annual totals)

# the definition of:
# pr = total annual precio
# tmmn = minimum of all annual daily minimum temperature 
# tmmx = maximum of all annual daily maximum temperature

vars<-c('pr', 'tmmn', 'tmmx')

vars_funs<-c(`sum`, `min`, `max`)

# set up a df for appending into each loop:

df.NWIS.Climate<-data.frame(ID = 1:dim(df.NWIS.CDL)[1])

i<-1

# loop through the climate variables in vars:

for (i in seq_along(vars)){
  
  # download a Spatraster for the entire bounding box of all the watersheds of the climate variable data for the loops iteration
  # each layer of the spatraster is a day for the climate variable in thedaterange provided inthe function call:
  
  climate<-getGridMET(vect.NWIS, varname = vars[i], startDate = "2016-01-01", endDate = "2019-12-31")[[1]]
  
  # reproject the watersheds to the crs of the climate variables raster:
  
  vect.NWIS.proj<-terra::project(vect.NWIS, crs(climate))
  
  # extract the daily mean of the climate variables data over the watersheds:
  # thisreturns a dataframe thefirstcolumn is the site (justnumbered 1-56 right now) and the other columns are the daily values of the climate vairable (taking the mean ofthe raster cells over the watershed): 
  
  climate <- terra::extract(climate, vect.NWIS.proj, mean)
  
  # reformat, convert dates, and calcualte annual stats:
  
  # pivot Longer: each of the columns for the daily values of the climate variable starts withthevariable name and an underscore, so using that pattern to make a long dataframe 
  # format the dates: the date is contained withinthe column that was priviously pivoted into a single column. so reformating this column and turing into a date:
  # grouping by the site and year, summarize the daily values into an annualvalue based onthe operation for the climatevariable. for pricep, sum is used, for min temp min is used, and formax temp max is used!:
  # rename the reulting year(Date) column to a nicer name:
  # renaming the values ofthe year column: add the climate variable to the year,this pairs withthe next andfinal step:
  # pivot wider to get unique columns for the years of the cimlatevariable (here its 2016-2019):
  
  climate1<-climate%>%
    pivot_longer(cols= starts_with(paste0(vars[i],"_")), names_to = 'Date',values_to = "Value")%>%
    mutate(Date=as.Date(str_replace(Date, paste0(vars[i],"_"), "")))%>%
    group_by(year(Date),ID)%>%
    summarize(Annual = vars_funs[[i]](Value))%>%
    rename(Year=1)%>%
    mutate(Year=paste0(vars[i],Year))%>%
    pivot_wider(names_from = Year, values_from = Annual)
  
  # merge the climate variables data to a common dataframe (initalized prior to loop).
  # New columns are added for each site:
  
  df.NWIS.Climate<-left_join(df.NWIS.Climate, climate1, by = 'ID')
  
}

# rename the sites:

df.NWIS.Climate[,1]<-df.NWIS.DEM$Name

# rename the sites:

names(df.NWIS.Climate)[1]<-'Name'

# save df:

save(df.NWIS.Climate, file = 'Processed_Data/df.NWIS.Climate.Rdata')

#























##### Determine if CAFO is in watershed ####

# read in CAFO locations and make sf dataframe

CAFOs<-read.csv('Raw_data/Copy of CAFO-list-from-NYSDEC March 2021 with lat long values.csv')%>%
  drop_na(longitude)%>%
  drop_na(latitude)%>%
  st_as_sf(., coords = c("longitude", "latitude"), crs = 4326)

# add CAFOS to watershed map:

# mapview(df.sf.NWIS.keep)+
#   mapview(CAFOs, zcol = 'SIZE')

# this file maps differently than the map on the DE website, but just going
# to proceed with this file for now

# determine how many CAFOs intersect each watershed:

df.sf.NWIS.keep$CAFO_count <- lengths(st_intersects(df.sf.NWIS.keep, CAFOs))

# make a map colored by CAFO count:

# mapview(df.sf.NWIS.keep, zcol = 'CAFO_count')

#































































####~~~~ Sufficentcy of Gauges 2 (NOT RUN) ~~~~####

# below is the code used when I was looking at the sufficency of gauges 2 sites
# regardless of the year of TP data. But filtring on 2001 above, I dont really care how many years of 
# data they have (right now at least...)

# # import gages 2 and filter to NYS TP CQ sites:
# 
# l.G2 <- read_excel_allsheets("Raw_Data/gagesII_sept30_2011_conterm_EDITED_again.xlsx")
# df.G2<-reduce(l.G2, full_join, by = "STAID")
# df.G2<-select(df.G2, STAID | where(is.numeric))
# df.G2<-df.G2%>%filter(STAID %in% TP_sites$site_no)
# 
# # add the n_sample_rank: to do this:
# # rerun the temp dataframe (since I keep writing over it in different sections:
# 
# temp<-df.NWIS.TP%>%group_by(site_no)%>%
#   summarise(n=n())%>%
#   arrange(desc(n))%>%
#   mutate(n_sample_rank=rank(-n, ties.method='first'))
# 
# # left join to add n_sample rank column:
# 
# df.G2<-left_join(df.G2, temp, by = c('STAID'='site_no'))
# 
# # I want look at the number of years of data for each of these:
# 
# G2_n_years<-filter(df.NWIS.TP_CQ, site_no %in% df.G2$STAID)%>%
#   group_by(site_no)%>%
#   summarize(min_date=min(year(sample_dt)),
#             max_date=max(year(sample_dt)),
#             n_years=max(year(sample_dt))-min(year(sample_dt)))%>%
#   arrange(desc(n_years))
# 
# # I want to look at the time series of the sites with little years of data: to do this:
# 
# # filter to sits with lessthan 10 years of data:
# 
# x<-filter(G2_n_years, n_years<10)
# 
# # plot:
# 
# p<-filter(df.NWIS.TP_CQ, site_no %in% x$site_no)%>%
#   # filter(., n_sample_rank==196)%>%
#   ggplot(., aes(x = as.Date(sample_dt), y = result_va))+
#   geom_point()+
#   facet_wrap('n_sample_rank', scales ='free')
# 
# # from this plot the following sites I say have good data even though they have less than 10 years of samples:
# 
# below_10_keep<-c(46,70,72,118,119,120,122,125,126,134)
# 
# # now come up with a final list of sites: to do this:
# 
# # merge the n_samplerank and n_years dfs and filter based on n_years and the numbers of n_sample_ranks in below_10_keep:
# 
# G2_keep<-left_join(G2_n_years, temp, by = 'site_no')%>%
#   filter(., n_years>=10 | n_sample_rank %in% below_10_keep)
# 
# # make a plot:
# 
# p1<-filter(df.NWIS.TP_CQ, site_no %in% G2_keep$site_no)%>%
#   # filter(., n_sample_rank==196)%>%
#   ggplot(., aes(x = sample_dt, y = result_va))+
#   facet_wrap('n_sample_rank', scales ='free')+
#   geom_point()
# 
# # now filter the gauges2 to these sites:
# 
# df.G2<-filter(df.G2, STAID %in% G2_keep$site_no)























































































#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
                     ####~~~~ MLR regular ~~~~####
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

# set up list of dataframes for different selection methods used below:

# create a list of dfs, one for each cQ parameter, with the values of the spearman correlations for each predictor:

l.cor.MLR<-df.cor %>%
  split(., df.cor$CQ_Parameter)%>%
  lapply(., \(i) i%>% 
           arrange(desc(abs(Spearman_Correlation)))%>%  
           # slice_head(n = 7)%>%
           # bind_rows(i%>%arrange(desc(Spearman_Correlation))%>%slice_head(n = 7))%>%
           # distinct(term, .keep_all = T)%>%
           # filter(!between(Spearman_Correlation, -0.25,.25))%>%
           mutate(sig_0.05 = factor(sig_0.05, levels = c('not', 'sig')))%>%
           # mutate(term = factor(term, levels = unique(term[order(Spearman_Correlation)])))%>%
           # filter(sig_0.05 == 'sig')%>%
           as.data.frame())


x<-l.cor.MLR[[1]]

# create a list of dfs from df.OLS.Sens, which contains the values of the predictors, subseting using the names in each l.cor[[i]]$term:

l.cor.MLR.full<-lapply(1:5, \(i) df.OLS_Sens%>%
                    select(i+1, l.cor.MLR[[i]]$term)%>%
                    as.data.frame()%>%
                      rename(term = 1))

names(l.cor.MLR.full)<-names(df.OLS_Sens)[2:6]

y<-l.cor.MLR.full[[1]]

y.round<-round(y, 2)

# create another list but keep the site names for use at end of loop in site outliers:

l.cor.MLR.full.w_name<-lapply(1:5, \(i) df.OLS_Sens%>%
                         select(1, i+1, l.cor.MLR[[i]]$term)%>%
                         as.data.frame())

names(l.cor.MLR.full.w_name)<-names(df.OLS_Sens)[2:6]

z<-l.cor.MLR.full.w_name[[1]]

# Trying no feature selection:

# loop through the 5 CQ parameters:

i<-1

for (i in 1:5){
  
  df<-l.cor.MLR.full[[i]]
  
  library(caret)
  
  set.seed(123)
  
  # Set up repeated k-fold cross-validation
  
  train.control <- trainControl(method = "cv", number = 10)
  
  # Train the model
  
  step.model <- train(term ~., data = df,
                      method = "leapForward", 
                      tuneGrid = data.frame(nvmax = 1:5),
                      trControl = train.control
  )
  

  # look at model results:
  
  step.model$results
  step.model$bestTune
  summary(step.model$finalModel)
  coef(step.model$finalModel, 4)
  
  # get df of just the predictors in this model:
  
  pred<-names(coef(step.model$finalModel, 4))[-1]
  
  df.2<-df%>%select(term, pred)
  
  # make lm:
  
  m<-lm(term~., data=df.2)
  
  summary(m)
  
  # look at univarite
  
  plots:



#

# loop:

i<-1

for (i in 1:5){
  
  # when i<-1, OLS.I
  
  # regardless of the method to pick the predictors for the MLR model, no more than 4 wil be included as per the run p < n/10
  
  #################################################
  # Spearman correlations::
  #################################################

  # build a lm using the top 4 correlates from the spearman correlaiton analysis:
  
  data.temp<-l.cor.MLR.full[[i]][,1:5]
  m.spear<-lm(term~., data = data.temp)
  
  summary(m.spear) # Adj R2 = 0.2121, pval = 0.01154, none of the terms signficant
  
  plot(m.spear) # 12, 22
  
  terms.m.spear<-names(m.spear$model)[-1]
  
  #################################################
  # Random Forest: 
  # changes everytime slightly even if seed is same, 
  # I think this is called unstable
  #################################################
  
  set.seed(1)
  
  # build RF model:
  x<-randomForest(term~., data = l.cor.MLR.full[[i]], na.action=na.omit)
  # arrange by RF variable importance 
  y<-tibble::rownames_to_column(as.data.frame(x$importance), "Attribute")%>%
    arrange(desc(IncNodePurity))
  # build lm of top 4:
  data.temp<-l.cor.MLR.full[[i]]%>%select(term, y$Attribute[1:4])
  m.rf<-lm(term~., data = data.temp)
  summary(m.rf) # adj R2 = 0.2494, pval = 0.005165, only RIP100_FOREST signficant
  plot(m.rf) # 22, 12
  
  terms.m.rf<-names(m.rf$model)[-1]
  
  #################################################
  # stepAIC:
  #################################################
  
  #### v1: full model:
  
  # stepAIC workflow:
  fit0 <- lm(term~1,data=l.cor.MLR.full[[i]])
  up.model <- paste("~", paste(colnames(l.cor.MLR.full[[i]][,-1]), collapse=" + "))
  m.step.all <- stepAIC(fit0,direction="both",scope=list(lower=fit0,upper=up.model),trace = FALSE)
  summary(m.step.all) # 
  
  #### v2: reduce predictor set prior to building full model:
  
  # scale variables because some have small ranges:
  
  data.temp<-l.cor.MLR.full[[i]][,-1]
  
  ### v2.1: check which predictors have the highest coefficent of variability among the 40 observations:
  
  v.mean<-sapply(l.cor.MLR.full[[i]][,-1], mean, na.rm=T)
  v.sd<-sapply(l.cor.MLR.full[[i]][,-1], sd, na.rm=T)
  v.cv<-v.mean/v.sd
  df.cv<-tibble::rownames_to_column(as.data.frame(v.cv), "Attribute")%>%
    rename(var=2)%>%
    arrange(desc(var))
  # the following top n-1 predictors are:
  df.cv$Attribute[1:39]
  # use stepAIC to build a 'full' model using the top n-1:
  fit0 <- lm(term~1,data=l.cor.MLR.full[[i]])
  up.model <- paste("~", paste(colnames(l.cor.MLR.full[[i]]%>%select(df.cv$Attribute[1:39])), collapse=" + "))
  m.step.39.cv <- stepAIC(fit0,direction="both",scope=list(lower=fit0,upper=up.model),trace = FALSE)
  summary(m.step.39.cv) # adj R2 = 0.7047, pval = 7.4e-08
  # this is pretty good but I still want p<n/10 
  # so use leaps to determine best 4:
  var.temp<-strsplit(as.character(m.step.39.cv$terms)[3],split=' + ', fixed=TRUE)[[1]]
  data.temp<-l.cor.MLR.full[[i]]%>%select(term, var.temp)
  m.leaps.step.cv.39to8to4<-regsubsets(term~.,data=data.temp,nbest=1)
  summary(m.leaps.step.cv.39to8to4)
  rownames(as.data.frame(coef(m.leaps.step.cv.39to8to4, 4)))[-1]
  # now build an lm with these 4:
  data.temp<-select(data.temp, term, rownames(as.data.frame(coef(m.leaps.step.cv.39to8to4, 4)))[-1])
  m.leaps.step.cv.39to8to4<-lm(term~., data=data.temp)
  summary(m.leaps.step.cv.39to8to4) # adj r2 = 0.5395, pval = 2.2e-06
  # plot(m.leaps.step.cv.39to8to4) # 39, 21, 23 (21 in cooks)
  
  terms.m.step.cv<-names(m.leaps.step.cv.39to8to4$model)[-1]
  
  ### v2.2: check which predictors have the highest variability among the 40 observations:
  
  v.var<-sapply(l.cor.MLR.full[[i]][,-1], var, na.rm=T)
  df.var<-tibble::rownames_to_column(as.data.frame(v.var), "Attribute")%>%
    rename(var=2)%>%
    arrange(desc(var))
  # the following top n-1 predictors are:
  df.var$Attribute[1:39]
  # build the full model using the top n-1:
  fit0 <- lm(term~1,data=l.cor.MLR.full[[i]])
  up.model <- paste("~", paste(colnames(l.cor.MLR.full[[i]]%>%select(df.var$Attribute[1:39])), collapse=" + "))
  m.step.39.var <- stepAIC(fit0,direction="both",scope=list(lower=fit0,upper=up.model),trace = FALSE)
  summary(m.step.39.var) # adj R2 = 0.9259, pval = 1.01e-08
  # this is amazing but we shouldn't use a model with 22 predictors
  # so use leaps to determine best 4:
  var.temp<-strsplit(as.character(m.step.39.var$terms)[3],split=' + ', fixed=TRUE)[[1]]
  data.temp<-l.cor.MLR.full[[i]]%>%select(term, var.temp)
  m.leaps.step.var.39to22to4<-regsubsets(term~.,data=data.temp,nbest=1)
  summary(m.leaps.step.var.39to22to4)
  rownames(as.data.frame(coef(m.leaps.step.var.39to22to4, 4)))[-1]
  # now build an lm with these 4:
  data.temp<-select(data.temp, term, rownames(as.data.frame(coef(m.leaps.step.var.39to22to4, 4)))[-1])
  m.leaps.step.var.39to22to4<-lm(term~., data=data.temp)
  summary(m.leaps.step.var.39to22to4) # adj r2 = 0.467, pval = 2.5e-05
  # plot(m.leaps.step.var.39to22to4) # 23, 25, 10, 21 (21 in cooks)
  
  terms.m.step.var<-names(m.leaps.step.var.39to22to4$model)[-1]
  
  #################################################
  # LEAPS:
  # (straight up with all 306 varables):
  #################################################
  
  # leaps: (doesnt run, too big)
  
  # fit.reg<-leaps::regsubsets(term~.,data=l.cor.MLR.full[[i]],nbest=4, really.big=T)
  
  #################################################
  # LASSO:
  # I dont know the difference between the two lasso methods I have below
  #################################################
  
  #### lasso v1: 
  
  # Define predictor and response variables
  y <- l.cor.MLR.full[[i]]$term
  x <- data.matrix(l.cor.MLR.full[[i]][, -1])
  #fit lasso regression model using k-fold cross-validation
  cv_model <- cv.glmnet(x, y, alpha = 1)
  best_lambda <- cv_model$lambda.min
  #display optimal lambda value
  best_lambda
  #view plot of test MSE's vs. lambda values
  plot(cv_model)
  #view coefficients of best model
  best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
  coef(best_model)
  # get non zero coeffs:
  Shat.v1 <- rownames(coef(best_model))[which(coef(best_model) != 0)]
  # since only 3 given, build lm with these!:
  data.temp<-select(l.cor.MLR.full[[i]], term, Shat.v1[-1])
  m.lasso.v1<-lm(term~., data=data.temp)
  summary(m.lasso.v1) # adj r2 = 0.376, pval = 0.00016
  plot(m.lasso.v1) # 23, 15, 21, 28 (28 in cooks)
  
  terms.m.lasso.v1<-names(m.lasso.v1$model)[-1]
  
  #### lasso v2:
  
  #Define predictor and response variables
  ytrain <- l.cor.MLR.full[[i]]$term
  xtrain <- data.matrix(l.cor.MLR.full[[i]][, -1])
  # fit model:
  fit.lasso.glmnet <-glmnet(x=xtrain,y=ytrain,alpha=1) 
  # fit cv (?)
  cv.lasso.glmnet <-cv.glmnet(x=xtrain,y=ytrain,alpha=1) 
  plot(cv.lasso.glmnet)
  # save beta coefficents (?)
  beta.lasso <- coef(fit.lasso.glmnet, s = cv.lasso.glmnet$lambda.min)
  beta.lasso
  # get non zero coeffs:
  Shat.v2 <- rownames(beta.lasso)[which(beta.lasso != 0)]
  # since only 4 given, build lm with these!:
  data.temp<-select(l.cor.MLR.full[[i]], term, Shat.v2[-1])
  m.lasso.v2<-lm(term~., data=data.temp)
  summary(m.lasso.v2) # adj r2 = 0.43, pval = 7.8e-05
  plot(m.lasso.v2) # 21, 39, 23, 28 (28 in cooks)
  
  terms.m.lasso.v2<-names(m.lasso.v2$model)[-1]

  #################################################
  # Correlations:
  #################################################

  # calculate correlation matrix
  correlationMatrix <- cor(l.cor.MLR.full[[i]][,-1])
  # summarize the correlation matrix
  print(correlationMatrix)
  # ggcorrplot(correlationMatrix)
  # find attributes that are highly corrected (ideally >0.75)
  highlyCorrelated <- caret::findCorrelation(correlationMatrix, cutoff=0.5)
  # print indexes of highly correlated attributes
  print(highlyCorrelated)
  unique(highlyCorrelated)
  # remove highly correlated variables:
  data.temp<-l.cor.MLR.full[[i]][,-c(highlyCorrelated+1)]
  # stepAIC workflow:
  fit0 <- lm(term~1,data=data.temp)
  up.model <- paste("~", paste(colnames(data.temp[,-1]), collapse=" + "))
  m.step.cor <- stepAIC(fit0,direction="both",scope=list(lower=fit0,upper=up.model),trace = FALSE)
  summary(m.step.cor) # adj r2=.4423,pval=0.0006
  # leaps workflow:
  var.temp<-strsplit(as.character(m.step.cor$terms)[3],split=' + ', fixed=TRUE)[[1]]
  data.temp<-l.cor.MLR.full[[i]]%>%select(term, var.temp)
  m.leaps.step.cor<-regsubsets(term~.,data=data.temp,nbest=1)
  summary(m.leaps.step.cor)
  rownames(as.data.frame(coef(m.leaps.step.cor, 4)))[-1]
  data.temp<-select(data.temp, term, rownames(as.data.frame(coef(m.leaps.step.cor, 4)))[-1])
  m.leaps.step.cor<-lm(term~., data=data.temp)
  summary(m.leaps.step.cor) # adj r2 = 0.2911, pval = 0.0027
  plot(m.leaps.step.cor) # 23, 1, 21 
  
  terms.m.cor<-names(m.leaps.step.cor$model)[-1]
  
  #################################################
  # Feature Importance
  #################################################
  
  control <- caret::trainControl(method="repeatedcv", number=10, repeats=3)
  # train the model
  model <- caret::train(term~., data=l.cor.MLR.full[[i]], method="leapSeq", preProcess="scale", trControl=control)
  # estimate variable importance
  importance <- varImp(model, scale=FALSE)
  # summarize importance
  print(importance)
  # plot importance
  plot(importance)
  # get top 4 most important:
  x<-tibble::rownames_to_column(importance$importance, "Attribute")%>%arrange(desc(Overall))
  # build lm with top 4:
  data.temp<-select(l.cor.MLR.full[[i]], term, x$Attribute[1:4])
  m.feat_import.4<-lm(term~., data= data.temp)
  summary(m.feat_import.4) # adj r2 = 0.2229, pval=0.01148
  plot(m.feat_import.4) # 23, 21, 11
  
  terms.m.feat_import<-names(m.feat_import.4$model)[-1]
  
  #################################################
  # Feature Selection:
  #################################################
  
  # define the control using a random forest selection function
  control <- caret::rfeControl(functions=rfFuncs, method="cv", number=10)
  # run the RFE algorithm
  results <- caret::rfe(l.cor.MLR.full[[i]][,-1], l.cor.MLR.full[[i]][,1], sizes=c(1:4), rfeControl=control)
  # summarize the results
  print(results)
  # list the chosen features
  predictors(results)
  # plot the results
  plot(results, type=c("g", "o"))
  # build lm with top 4:
  x<-predictors(results)[1:4]
  # build lm with top 4:
  data.temp<-select(l.cor.MLR.full[[i]], term, x)
  m.feat_select.4<-lm(term~., data= data.temp)
  summary(m.feat_select.4) # adj r2 = 0.244, pval=0.0074
  plot(m.feat_select.4) # 23, 21, 27
  
  terms.m.feat_select<-names(m.feat_select.4$model)[-1]
  
  #################################################
  # PCA:
  # http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/
  #################################################
  
  data.temp<-l.cor.MLR.full[[i]][,-1]
  res.pca <- PCA(data.temp, graph = FALSE)
  print(res.pca)
  eig.val <- get_eigenvalue(res.pca)
  eig.val # sum(eig.val[,1]) # The sum of all the eigenvalues give a total variance of p (305 here).
  fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))
  var <- get_pca_var(res.pca)
  var
  fviz_pca_var(res.pca, col.var = "black")
  library("corrplot")
  corrplot(var$cos2, is.corr=FALSE)
  fviz_cos2(res.pca, choice = "var", axes = 1:2)
  fviz_pca_var(res.pca, col.var = "cos2",gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE) # Avoid text overlapping)
  fviz_pca_var(res.pca, alpha.var = "cos2")
  corrplot(var$contrib, is.corr=FALSE) 
  # Contributions of variables to PC1
  fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
  # Contributions of variables to PC2
  fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
  # The total contribution to PC1 and PC2 is obtained with the following R code:
  fviz_contrib(res.pca, choice = "var", axes = 1:2, top = 10)
  # The most important (or, contributing) variables can be highlighted on the correlation plot as follow:
  fviz_pca_var(res.pca, col.var = "contrib",gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
  # Create a random continuous variable of length 10
  set.seed(123)
  my.cont.var <- rnorm(305)
  # Color variables by the continuous variable
  fviz_pca_var(res.pca, col.var = my.cont.var,gradient.cols = c("blue", "yellow", "red"),legend.title = "Cont.Var")
  # Create a grouping variable using kmeans
  # Create 3 groups of variables (centers = 3)
  set.seed(123)
  res.km <- kmeans(var$coord, centers = 3, nstart = 25)
  grp <- as.factor(res.km$cluster)
  # Color variables by groups
  fviz_pca_var(res.pca, col.var = grp, palette = c("#0073C2FF", "#EFC000FF", "#868686FF"),legend.title = "Cluster")
  # graph of individuals
  fviz_pca_ind(res.pca, col.ind = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE) # Avoid text overlapping (slow if many points))
  # You can also change the point size according the cos2 of the corresponding individuals:
  fviz_pca_ind(res.pca, pointsize = "cos2", pointshape = 21, fill = "#E7B800",repel = TRUE) # Avoid text overlapping (slow if many points)
  # Total contribution on PC1 and PC2
  fviz_contrib(res.pca, choice = "ind", axes = 1:2)
  # biplot:
  fviz_pca_biplot(res.pca, repel = TRUE,col.var = "#2E9FDF", col.ind = "#696969"  )
  
  # Im just going to stop with PCA for now because I dont know how to interpret results:
  
  #################################################
  # Looking at regression outliers:
  #################################################
  
  # The following observations (1-40) came up in the plot(lm) calls throughout thecode above for i=1 (OLS.I):
  # n times (second number)
  
  x<-c(1, 1, 10, 1 , 11, 2, 15, 1, 21, 9, 23, 9, 25, 1, 27, 1, 28, 2, 37, 2, 39, 2)
  x<-as.data.frame(do.call(cbind, split(x, c("Site", "n")))) %>% separate_rows("n", sep = ",")%>%arrange(n)
  
  # more than any others, sites 21 and 23 kept coming up as outliers
  # look at these sites. to do this:
  
  look_at_sites<-l.cor.MLR.full.w_name[[i]][c(21,23),]$Name 
  
  map.1<-df.sf.NWIS%>%filter(Name %in% look_at_sites)
    
  mapview(map.1)
  
  # if I look at the facet plot for CQ type and land use, nothing stands out 
  # about these problem sites. I mean, two are ag and two are undeveloped, but I 
  # dont trust the land use classifications. Also two classes out of 4 isnt that impressive. 
  
  #################################################
  # Looking at frequcny of terms across the models:
  #################################################
  
  x<-grep("terms",names(.GlobalEnv),value=TRUE)
  x<-do.call("list",mget(Pattern1))
  x<-unlist(x)
  x<-as.data.frame(table(x))%>%arrange(desc(Freq))
  
  #################################################
  # Building MLR with handpicked predictors, the ones that capture disturbance:
  #################################################
  
  # the following gauges 2 attributes are kept:
  
  pred_to_keep<-c("term", 
                  "HYDRO_DISTURB_INDX", 
                  "pre1990_STOR", 
                  names(l.G2$Landscape_Pat), 
                  names(l.G2$LC06_Basin),
                  names(l.G2$LC_Crops),
                  'HGA', 'HGB', 'HGC', 'HGD',
                  names(l.G2$Topo)[!grepl("ASPECT|MEAN", names(l.G2$Topo))]
                  )
  
  # I orginally also included in pred_to_keep:
  
  # names(select(l.G2$Climate, ends_with('BASIN'))), names(l.G2$Topo), "BAS_COMPACTNESS", "DRAIN_SQKM", "BFI_AVE","TOPWET"
  
  # removing duplicates of STADID:
  
  pred_to_keep <- pred_to_keep[!(pred_to_keep %in% "STAID")]
  
  # removing predictors not in the correlaiton df columns (some didnt make it):
  
  pred_to_keep <- pred_to_keep[(pred_to_keep %in% names(l.cor.MLR.full[[i]]))]
  
  # filter the correlation df to these predictors

  data.temp<-l.cor.MLR.full[[i]]%>%select(pred_to_keep)

  # the two stepAIC parts from above had really good adj R2 so starting with those to see
  # how model building with these predictor set does:
  
  ### v2.1: check which predictors have the highest coefficent of variability among the 40 observations:
  
  v.mean<-sapply(data.temp[,-1], mean, na.rm=T)
  v.sd<-sapply(data.temp[,-1], sd, na.rm=T)
  v.cv<-v.mean/v.sd
  df.cv<-tibble::rownames_to_column(as.data.frame(v.cv), "Attribute")%>%
    rename(var=2)%>%
    arrange(desc(var))
  # the following top n-1 predictors are:
  df.cv$Attribute[1:39]
  # use stepAIC to build a 'full' model using the top n-1:
  fit0 <- lm(term~1,data=data.temp)
  up.model <- paste("~", paste(colnames(data.temp%>%select(df.cv$Attribute[1:39])), collapse=" + "))
  m.step.39.cv <- stepAIC(fit0,direction="both",scope=list(lower=fit0,upper=up.model),trace = FALSE)
  summary(m.step.39.cv) # adj R2 = 0.7909, pval = 2.076e-05
  # this is pretty good but I still want p<n/10 
  # so use leaps to determine best 4:
  var.temp<-strsplit(as.character(m.step.39.cv$terms)[3],split=' + ', fixed=TRUE)[[1]]
  data.temp<-data.temp%>%select(term, var.temp)
  m.leaps.step.cv.39to8to4<-regsubsets(term~.,data=data.temp,nbest=1)
  summary(m.leaps.step.cv.39to8to4)
  rownames(as.data.frame(coef(m.leaps.step.cv.39to8to4, 4)))[-1] 
  # now build an lm with these 4:
  data.temp<-select(data.temp, term, rownames(as.data.frame(coef(m.leaps.step.cv.39to8to4, 4)))[-1])
  m.leaps.step.cv.39to8to4<-lm(term~., data=data.temp)
  summary(m.leaps.step.cv.39to8to4) # adj r2 = 0.3339, pval = 0.0009889
  # plot(m.leaps.step.cv.39to8to4) # 39, 21, 23 (21 in cooks)
  
  terms.m.step.cv<-names(m.leaps.step.cv.39to8to4$model)[-1]
  
  ### v2.2: check which predictors have the highest variability among the 40 observations:
  
  # need to recreate data.temp:
  
  data.temp<-l.cor.MLR.full[[i]]%>%select(pred_to_keep)
  
  v.var<-sapply(data.temp[,-1], var, na.rm=T)
  df.var<-tibble::rownames_to_column(as.data.frame(v.var), "Attribute")%>%
    rename(var=2)%>%
    arrange(desc(var))
  # the following top n-1 predictors are:
  df.var$Attribute[1:39]
  # build the full model using the top n-1:
  fit0 <- lm(term~1,data=data.temp)
  up.model <- paste("~", paste(colnames(data.temp%>%select(df.var$Attribute[1:39])), collapse=" + "))
  m.step.39.var <- stepAIC(fit0,direction="both",scope=list(lower=fit0,upper=up.model),trace = FALSE)
  summary(m.step.39.var) # adj R2 = 0.9259, pval = 1.01e-08
  # this is amazing but we shouldn't use a model with 22 predictors
  # so use leaps to determine best 4:
  var.temp<-strsplit(as.character(m.step.39.var$terms)[3],split=' + ', fixed=TRUE)[[1]]
  data.temp<-data.temp%>%select(term, var.temp)
  m.leaps.step.var.39to22to4<-regsubsets(term~.,data=data.temp,nbest=1)
  summary(m.leaps.step.var.39to22to4)
  rownames(as.data.frame(coef(m.leaps.step.var.39to22to4, 4)))[-1]
  # now build an lm with these 4:
  data.temp<-select(data.temp, term, rownames(as.data.frame(coef(m.leaps.step.var.39to22to4, 4)))[-1])
  m.leaps.step.var.39to22to4<-lm(term~., data=data.temp)
  summary(m.leaps.step.var.39to22to4) # adj r2 = 0.3967 , pval = 0.0001962
  # plot(m.leaps.step.var.39to22to4) # 23, 25, 10, 21 (21 in cooks)
  
  terms.m.step.var<-names(m.leaps.step.var.39to22to4$model)[-1]
  
  #################################################
  # standardize predictors
  #################################################
  
  # just running it on the reduced dataset for disturbance predictors:
  
  # need to recreate data.temp and standarize:
  
  data.temp<-l.cor.MLR.full[[i]]%>%select(pred_to_keep)%>%mutate(across(where(is.numeric), ~c(scale(.))))
  
  # now rerun stepAIC+leaps feature selection workflows o thisdataframe:
  
  ### v2.1: check which predictors have the highest coefficent of variability among the 40 observations:
  
  v.mean<-sapply(data.temp[,-1], mean, na.rm=T)
  v.sd<-sapply(data.temp[,-1], sd, na.rm=T)
  v.cv<-v.mean/v.sd
  df.cv<-tibble::rownames_to_column(as.data.frame(v.cv), "Attribute")%>%
    rename(var=2)%>%
    arrange(desc(var))
  # the following top n-1 predictors are:
  df.cv$Attribute[1:39]
  # use stepAIC to build a 'full' model using the top n-1:
  fit0 <- lm(term~1,data=data.temp)
  up.model <- paste("~", paste(colnames(data.temp%>%select(df.cv$Attribute[1:39])), collapse=" + "))
  m.step.39.cv <- stepAIC(fit0,direction="both",scope=list(lower=fit0,upper=up.model),trace = FALSE)
  summary(m.step.39.cv) # adj R2 = 0.7909, pval = 2.076e-05
  # this is pretty good but I still want p<n/10 
  # so use leaps to determine best 4:
  var.temp<-strsplit(as.character(m.step.39.cv$terms)[3],split=' + ', fixed=TRUE)[[1]]
  data.temp<-data.temp%>%select(term, var.temp)
  m.leaps.step.cv.39to8to4<-regsubsets(term~.,data=data.temp,nbest=1)
  summary(m.leaps.step.cv.39to8to4)
  rownames(as.data.frame(coef(m.leaps.step.cv.39to8to4, 4)))[-1] 
  # now build an lm with these 4:
  data.temp<-select(data.temp, term, rownames(as.data.frame(coef(m.leaps.step.cv.39to8to4, 4)))[-1])
  m.leaps.step.cv.39to8to4<-lm(term~., data=data.temp)
  summary(m.leaps.step.cv.39to8to4) # adj r2 = 0.3339, pval = 0.0009889
  # plot(m.leaps.step.cv.39to8to4) # 39, 21, 23 (21 in cooks)
  
  terms.m.step.cv<-names(m.leaps.step.cv.39to8to4$model)[-1]
  
  ### v2.2: check which predictors have the highest variability among the 40 observations:
  
  # need to recreate data.temp:
  
  data.temp<-l.cor.MLR.full[[i]]%>%select(pred_to_keep)
  
  v.var<-sapply(data.temp[,-1], var, na.rm=T)
  df.var<-tibble::rownames_to_column(as.data.frame(v.var), "Attribute")%>%
    rename(var=2)%>%
    arrange(desc(var))
  # the following top n-1 predictors are:
  df.var$Attribute[1:39]
  # build the full model using the top n-1:
  fit0 <- lm(term~1,data=data.temp)
  up.model <- paste("~", paste(colnames(data.temp%>%select(df.var$Attribute[1:39])), collapse=" + "))
  m.step.39.var <- stepAIC(fit0,direction="both",scope=list(lower=fit0,upper=up.model),trace = FALSE)
  summary(m.step.39.var) # adj R2 = 0.9259, pval = 1.01e-08
  # this is amazing but we shouldn't use a model with 22 predictors
  # so use leaps to determine best 4:
  var.temp<-strsplit(as.character(m.step.39.var$terms)[3],split=' + ', fixed=TRUE)[[1]]
  data.temp<-data.temp%>%select(term, var.temp)
  m.leaps.step.var.39to22to4<-regsubsets(term~.,data=data.temp,nbest=1)
  summary(m.leaps.step.var.39to22to4)
  rownames(as.data.frame(coef(m.leaps.step.var.39to22to4, 4)))[-1]
  # now build an lm with these 4:
  data.temp<-select(data.temp, term, rownames(as.data.frame(coef(m.leaps.step.var.39to22to4, 4)))[-1])
  m.leaps.step.var.39to22to4<-lm(term~., data=data.temp)
  summary(m.leaps.step.var.39to22to4) # adj r2 = 0.3967 , pval = 0.0001962
  # plot(m.leaps.step.var.39to22to4) # 23, 25, 10, 21 (21 in cooks)
  
  terms.m.step.var<-names(m.leaps.step.var.39to22to4$model)[-1]
  
  
  
  
  
  
#########
  
}


# summary table of models: to do this:

# create a list of model names from gloabl enviornment:

rm(m.simple)

l.m<-names(.GlobalEnv) %>% str_subset(pattern = "^m\\.")

# put the models by their model names into a list:

l.m<-do.call("list",mget(l.m))

# filter the list to those with 5 or less terms (1 for intercept):

l.m<-l.m[sapply(l.m, \(i) ifelse(length(coef(i))<6, T, F))]

tab_model(l.m, dv.labels = names(l.m), file="MLR.tab_model.OLS.I.html")

#


############################################################################
                    ####~~~~ MLR standardized ~~~~####
############################################################################

# set up list of dataframes for different selection methods used below:

# create the l..cor list as above but now only filter out the 
# non-significant attributes:

l.cor.MLR<-df.cor %>%
  split(., df.cor$CQ_Parameter)%>%
  lapply(., \(i) i%>% 
           arrange(Spearman_Correlation)%>%  
           # slice_head(n = 7)%>%
           # bind_rows(i%>%arrange(desc(Spearman_Correlation))%>%slice_head(n = 7))%>%
           # distinct(term, .keep_all = T)%>%
           # filter(!between(Spearman_Correlation, -0.25,.25))%>%
           mutate(sig_0.05 = factor(sig_0.05, levels = c('not', 'sig')))%>%
           # mutate(term = factor(term, levels = unique(term[order(Spearman_Correlation)])))%>%
           # filter(sig_0.05 == 'sig')%>%
           as.data.frame())


# create a list of dataframes from df.OLS.Sens and subseting the attributes using the 
# names in each l.cor[[i]]$term for simple and full models:

l.cor.MLR.full<-lapply(1:4, \(i) df.OLS_Sens%>%
                         select(i+1, l.cor.MLR[[i]]$term)%>%
                         as.data.frame()%>%
                         rename(term = 1))

# standardize predictors (use same name so code infor loop doesnthaveto be changed)

l.cor.MLR.full<-lapply(l.cor.MLR.full, \(i) i%>%mutate(across(where(is.numeric), scale)))


# create another list but keep the site names for use at end of loop in site outliers:

l.cor.MLR.full.w_name<-lapply(1:4, \(i) df.OLS_Sens%>%
                                select(1, i+1, l.cor.MLR[[i]]$term)%>%
                                as.data.frame())

x<-l.cor.MLR.full.w_name[[4]]

# 

# loop:

i<-1

for (i in 1:4){
  
  # regardless of the method to pick the predictors for the MLR model, no more than 4 wil be included as per the run p < n/10
  
  #################################################
  # Spearman correlations::
  #################################################
  
  # build a lm using the top 4 correlates from the spearman correlaiton analysis:
  
  data.temp<-l.cor.MLR.full[[i]][,1:5]
  m.spear<-lm(term~., data = data.temp)
  
  summary(m.spear) # Adj R2 = 0.2074, pval = 0.0156
  
  # plot(m.spear) # 21, 37,23
  
  terms.m.spear<-names(m.spear$model)[-1]
  
  #################################################
  # Random Forest: 
  # changes everytime slightly even if seed is same, 
  # I think this is called unstable
  #################################################
  
  set.seed(1)
  
  # build RF model:
  x<-randomForest(term~., data = l.cor.MLR.full[[i]], na.action=na.omit)
  # arrange by RF variable importance 
  y<-tibble::rownames_to_column(as.data.frame(x$importance), "Attribute")%>%
    arrange(desc(IncNodePurity))
  # build lm of top 4:
  data.temp<-l.cor.MLR.full[[i]]%>%select(term, y$Attribute[1:4])
  m.rf<-lm(term~., data = data.temp)
  summary(m.rf) # adj R2 = 0.2444, pval = 0.0073
  plot(m.rf) # 11, 21, 23, 37 (in cooks)
  
  terms.m.rf<-names(m.rf$model)[-1]
  
  #################################################
  # stepAIC:
  #################################################
  
  #### v1: full model:
  
  # stepAIC workflow:
  fit0 <- lm(term~1,data=l.cor.MLR.full[[i]])
  up.model <- paste("~", paste(colnames(l.cor.MLR.full[[i]][,-1]), collapse=" + "))
  m.step.all <- stepAIC(fit0,direction="both",scope=list(lower=fit0,upper=up.model),trace = FALSE)
  summary(m.step.all) # doesnt work
  
  #### v2: reduce predictor set prior to building full model:
  
  # scale variables because some have small ranges:
  
  data.temp<-l.cor.MLR.full[[i]][,-1]
  
  ### v2.1: check which predictors have the highest coefficent of variability among the 40 observations:
  
  v.mean<-sapply(l.cor.MLR.full[[i]][,-1], mean, na.rm=T)
  v.sd<-sapply(l.cor.MLR.full[[i]][,-1], sd, na.rm=T)
  v.cv<-v.mean/v.sd
  df.cv<-tibble::rownames_to_column(as.data.frame(v.cv), "Attribute")%>%
    rename(var=2)%>%
    arrange(desc(var))
  # the following top n-1 predictors are:
  df.cv$Attribute[1:39]
  # use stepAIC to build a 'full' model using the top n-1:
  fit0 <- lm(term~1,data=l.cor.MLR.full[[i]])
  up.model <- paste("~", paste(colnames(l.cor.MLR.full[[i]]%>%select(df.cv$Attribute[1:39])), collapse=" + "))
  m.step.39.cv <- stepAIC(fit0,direction="both",scope=list(lower=fit0,upper=up.model),trace = FALSE)
  summary(m.step.39.cv) # adj R2 = 0.75
  # this is pretty good but I still want p<n/10 
  # so use leaps to determine best 4:
  var.temp<-strsplit(as.character(m.step.39.cv$terms)[3],split=' + ', fixed=TRUE)[[1]]
  data.temp<-l.cor.MLR.full[[i]]%>%select(term, var.temp)
  m.leaps.step.cv.39to8to4<-regsubsets(term~.,data=data.temp,nbest=1)
  summary(m.leaps.step.cv.39to8to4)
  rownames(as.data.frame(coef(m.leaps.step.cv.39to8to4, 4)))[-1]
  # now build an lm with these 4:
  data.temp<-select(data.temp, term, rownames(as.data.frame(coef(m.leaps.step.cv.39to8to4, 4)))[-1])
  m.leaps.step.cv.39to8to4<-lm(term~., data=data.temp)
  summary(m.leaps.step.cv.39to8to4) # adj r2 = 0.5395, pval = 2.2e-06
  plot(m.leaps.step.cv.39to8to4) # 39, 21, 23 (21 in cooks)
  
  terms.m.step.cv<-names(m.leaps.step.cv.39to8to4$model)[-1]
  
  ### v2.2: check which predictors have the highest variability among the 40 observations:
  
  v.var<-sapply(l.cor.MLR.full[[i]][,-1], var, na.rm=T)
  df.var<-tibble::rownames_to_column(as.data.frame(v.var), "Attribute")%>%
    rename(var=2)%>%
    arrange(desc(var))
  # the following top n-1 predictors are:
  df.var$Attribute[1:39]
  # build the full model using the top n-1:
  fit0 <- lm(term~1,data=l.cor.MLR.full[[i]])
  up.model <- paste("~", paste(colnames(l.cor.MLR.full[[i]]%>%select(df.var$Attribute[1:39])), collapse=" + "))
  m.step.39.var <- stepAIC(fit0,direction="both",scope=list(lower=fit0,upper=up.model),trace = FALSE)
  summary(m.step.39.var) # adj R2 = 0.9259, pval = 1.01e-08
  # this is amazing but we shouldn't use a model with 22 predictors
  # so use leaps to determine best 4:
  var.temp<-strsplit(as.character(m.step.39.var$terms)[3],split=' + ', fixed=TRUE)[[1]]
  data.temp<-l.cor.MLR.full[[i]]%>%select(term, var.temp)
  m.leaps.step.var.39to22to4<-regsubsets(term~.,data=data.temp,nbest=1)
  summary(m.leaps.step.var.39to22to4)
  rownames(as.data.frame(coef(m.leaps.step.var.39to22to4, 4)))[-1]
  # now build an lm with these 4:
  data.temp<-select(data.temp, term, rownames(as.data.frame(coef(m.leaps.step.var.39to22to4, 4)))[-1])
  m.leaps.step.var.39to22to4<-lm(term~., data=data.temp)
  summary(m.leaps.step.var.39to22to4) # adj r2 = 0.467, pval = 2.5e-05
  plot(m.leaps.step.var.39to22to4) # 23, 25, 10, 21 (21 in cooks)
  
  terms.m.step.var<-names(m.leaps.step.var.39to22to4$model)[-1]
  
  #################################################
  # LEAPS:
  # (straight up with all 306 varables):
  #################################################
  
  # leaps: (doesnt run, too big)
  
  # fit.reg<-leaps::regsubsets(term~.,data=l.cor.MLR.full[[i]],nbest=4, really.big=T)
  
  #################################################
  # LASSO:
  # I dont know the difference between the two lasso methods I have below
  #################################################
  
  #### lasso v1: 
  
  # Define predictor and response variables
  y <- l.cor.MLR.full[[i]]$term
  x <- data.matrix(l.cor.MLR.full[[i]][, -1])
  #fit lasso regression model using k-fold cross-validation
  cv_model <- cv.glmnet(x, y, alpha = 1)
  best_lambda <- cv_model$lambda.min
  #display optimal lambda value
  best_lambda
  #view plot of test MSE's vs. lambda values
  plot(cv_model)
  #view coefficients of best model
  best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
  coef(best_model)
  # get non zero coeffs:
  Shat.v1 <- rownames(coef(best_model))[which(coef(best_model) != 0)]
  # since only 3 given, build lm with these!:
  data.temp<-select(l.cor.MLR.full[[i]], term, Shat.v1[-1])
  m.lasso.v1<-lm(term~., data=data.temp)
  summary(m.lasso.v1) # adj r2 = 0.376, pval = 0.00016
  plot(m.lasso.v1) # 23, 15, 21, 28 (28 in cooks)
  
  terms.m.lasso.v1<-names(m.lasso.v1$model)[-1]
  
  #### lasso v2:
  
  #Define predictor and response variables
  ytrain <- l.cor.MLR.full[[i]]$term
  xtrain <- data.matrix(l.cor.MLR.full[[i]][, -1])
  # fit model:
  fit.lasso.glmnet <-glmnet(x=xtrain,y=ytrain,alpha=1) 
  # fit cv (?)
  cv.lasso.glmnet <-cv.glmnet(x=xtrain,y=ytrain,alpha=1) 
  plot(cv.lasso.glmnet)
  # save beta coefficents (?)
  beta.lasso <- coef(fit.lasso.glmnet, s = cv.lasso.glmnet$lambda.min)
  beta.lasso
  # get non zero coeffs:
  Shat.v2 <- rownames(beta.lasso)[which(beta.lasso != 0)]
  # since only 4 given, build lm with these!:
  data.temp<-select(l.cor.MLR.full[[i]], term, Shat.v2[-1])
  m.lasso.v2<-lm(term~., data=data.temp)
  summary(m.lasso.v2) # adj r2 = 0.43, pval = 7.8e-05
  plot(m.lasso.v2) # 21, 39, 23, 28 (28 in cooks)
  
  terms.m.lasso.v2<-names(m.lasso.v2$model)[-1]
  
  #################################################
  # Correlations:
  #################################################
  
  # calculate correlation matrix
  correlationMatrix <- cor(l.cor.MLR.full[[i]][,-1])
  # summarize the correlation matrix
  print(correlationMatrix)
  # ggcorrplot(correlationMatrix)
  # find attributes that are highly corrected (ideally >0.75)
  highlyCorrelated <- caret::findCorrelation(correlationMatrix, cutoff=0.5)
  # print indexes of highly correlated attributes
  print(highlyCorrelated)
  unique(highlyCorrelated)
  # remove highly correlated variables:
  data.temp<-l.cor.MLR.full[[i]][,-c(highlyCorrelated+1)]
  # stepAIC workflow:
  fit0 <- lm(term~1,data=data.temp)
  up.model <- paste("~", paste(colnames(data.temp[,-1]), collapse=" + "))
  m.step.cor <- stepAIC(fit0,direction="both",scope=list(lower=fit0,upper=up.model),trace = FALSE)
  summary(m.step.cor) # adj r2=.4423,pval=0.0006
  # leaps workflow:
  var.temp<-strsplit(as.character(m.step.cor$terms)[3],split=' + ', fixed=TRUE)[[1]]
  data.temp<-l.cor.MLR.full[[i]]%>%select(term, var.temp)
  m.leaps.step.cor<-regsubsets(term~.,data=data.temp,nbest=1)
  summary(m.leaps.step.cor)
  rownames(as.data.frame(coef(m.leaps.step.cor, 4)))[-1]
  data.temp<-select(data.temp, term, rownames(as.data.frame(coef(m.leaps.step.cor, 4)))[-1])
  m.leaps.step.cor<-lm(term~., data=data.temp)
  summary(m.leaps.step.cor) # adj r2 = 0.2911, pval = 0.0027
  plot(m.leaps.step.cor) # 23, 1, 21 
  
  terms.m.cor<-names(m.leaps.step.cor$model)[-1]
  
  #################################################
  # Feature Importance
  #################################################
  
  control <- caret::trainControl(method="repeatedcv", number=10, repeats=3)
  # train the model
  model <- caret::train(term~., data=l.cor.MLR.full[[i]], method="leapSeq", preProcess="scale", trControl=control)
  # estimate variable importance
  importance <- varImp(model, scale=FALSE)
  # summarize importance
  print(importance)
  # plot importance
  plot(importance)
  # get top 4 most important:
  x<-tibble::rownames_to_column(importance$importance, "Attribute")%>%arrange(desc(Overall))
  # build lm with top 4:
  data.temp<-select(l.cor.MLR.full[[i]], term, x$Attribute[1:4])
  m.feat_import.4<-lm(term~., data= data.temp)
  summary(m.feat_import.4) # adj r2 = 0.2229, pval=0.01148
  plot(m.feat_import.4) # 23, 21, 11
  
  terms.m.feat_import<-names(m.feat_import.4$model)[-1]
  
  #################################################
  # Feature Selection:
  #################################################
  
  # define the control using a random forest selection function
  control <- caret::rfeControl(functions=rfFuncs, method="cv", number=10)
  # run the RFE algorithm
  results <- caret::rfe(l.cor.MLR.full[[i]][,-1], l.cor.MLR.full[[i]][,1], sizes=c(1:4), rfeControl=control)
  # summarize the results
  print(results)
  # list the chosen features
  predictors(results)
  # plot the results
  plot(results, type=c("g", "o"))
  # build lm with top 4:
  x<-predictors(results)[1:4]
  # build lm with top 4:
  data.temp<-select(l.cor.MLR.full[[i]], term, x)
  m.feat_select.4<-lm(term~., data= data.temp)
  summary(m.feat_select.4) # adj r2 = 0.244, pval=0.0074
  plot(m.feat_select.4) # 23, 21, 27
  
  terms.m.feat_select<-names(m.feat_select.4$model)[-1]
  
  #################################################
  # PCA:
  # http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/
  #################################################
  
  data.temp<-l.cor.MLR.full[[i]][,-1]
  res.pca <- PCA(data.temp, graph = FALSE)
  print(res.pca)
  eig.val <- get_eigenvalue(res.pca)
  eig.val # sum(eig.val[,1]) # The sum of all the eigenvalues give a total variance of p (305 here).
  fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))
  var <- get_pca_var(res.pca)
  var
  fviz_pca_var(res.pca, col.var = "black")
  library("corrplot")
  corrplot(var$cos2, is.corr=FALSE)
  fviz_cos2(res.pca, choice = "var", axes = 1:2)
  fviz_pca_var(res.pca, col.var = "cos2",gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE) # Avoid text overlapping)
  fviz_pca_var(res.pca, alpha.var = "cos2")
  corrplot(var$contrib, is.corr=FALSE) 
  # Contributions of variables to PC1
  fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
  # Contributions of variables to PC2
  fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
  # The total contribution to PC1 and PC2 is obtained with the following R code:
  fviz_contrib(res.pca, choice = "var", axes = 1:2, top = 10)
  # The most important (or, contributing) variables can be highlighted on the correlation plot as follow:
  fviz_pca_var(res.pca, col.var = "contrib",gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
  # Create a random continuous variable of length 10
  set.seed(123)
  my.cont.var <- rnorm(305)
  # Color variables by the continuous variable
  fviz_pca_var(res.pca, col.var = my.cont.var,gradient.cols = c("blue", "yellow", "red"),legend.title = "Cont.Var")
  # Create a grouping variable using kmeans
  # Create 3 groups of variables (centers = 3)
  set.seed(123)
  res.km <- kmeans(var$coord, centers = 3, nstart = 25)
  grp <- as.factor(res.km$cluster)
  # Color variables by groups
  fviz_pca_var(res.pca, col.var = grp, palette = c("#0073C2FF", "#EFC000FF", "#868686FF"),legend.title = "Cluster")
  # graph of individuals
  fviz_pca_ind(res.pca, col.ind = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE) # Avoid text overlapping (slow if many points))
  # You can also change the point size according the cos2 of the corresponding individuals:
  fviz_pca_ind(res.pca, pointsize = "cos2", pointshape = 21, fill = "#E7B800",repel = TRUE) # Avoid text overlapping (slow if many points)
  # Total contribution on PC1 and PC2
  fviz_contrib(res.pca, choice = "ind", axes = 1:2)
  # biplot:
  fviz_pca_biplot(res.pca, repel = TRUE,col.var = "#2E9FDF", col.ind = "#696969"  )
  
  # Im just going to stop with PCA for now because I dont know how to interpret results:
  
  #################################################
  # Looking at regression outliers:
  #################################################
  
  # The following observations (1-40) came up in the plot(lm) calls throughout thecode above for i=1 (OLS.I):
  # n times (second number)
  
  x<-c(1, 1, 10, 1 , 11, 2, 15, 1, 21, 9, 23, 9, 25, 1, 27, 1, 28, 2, 37, 2, 39, 2)
  x<-as.data.frame(do.call(cbind, split(x, c("Site", "n")))) %>% separate_rows("n", sep = ",")%>%arrange(n)
  
  # more than any others, sites 21 and 23 kept coming up as outliers
  # look at these sites. to do this:
  
  look_at_sites<-l.cor.MLR.full.w_name[[i]][c(21,23),]$Name 
  
  map.1<-df.sf.NWIS%>%filter(Name %in% look_at_sites)
  
  mapview(map.1)
  
  # if I look at the facet plot for CQ type and land use, nothing stands out 
  # about these problem sites. I mean, two are ag and two are undeveloped, but I 
  # dont trust the land use classifications. Also two classes out of 4 isnt that impressive. 
  
  #################################################
  # Looking at frequcny of terms across the models:
  #################################################
  
  x<-grep("terms",names(.GlobalEnv),value=TRUE)
  x<-do.call("list",mget(Pattern1))
  x<-unlist(x)
  x<-as.data.frame(table(x))%>%arrange(desc(Freq))
  
  #################################################
  # Building MLR with handpicked predictors, the ones that capture disturbance:
  #################################################
  
  # the following gauges 2 attributes are kept:
  
  pred_to_keep<-c("term", "HYDRO_DISTURB_INDX", "BAS_COMPACTNESS", "DRAIN_SQKM", names(select(l.G2$Climate, ends_with('BASIN'))),
                  "BFI_AVE", "TOPWET", "pre1990_STOR", names(l.G2$Landscape_Pat), names(l.G2$LC06_Basin),names(l.G2$LC_Crops), names(l.G2$Topo))
  
  # removing duplicates of STADID:
  
  pred_to_keep <- pred_to_keep[!(pred_to_keep %in% "STAID")]
  
  # removing predictors not in the correlaiton df columns (some didnt make it):
  
  pred_to_keep <- pred_to_keep[(pred_to_keep %in% names(l.cor.MLR.full[[i]]))]
  
  # filter the correlation df to these predictors
  
  data.temp<-l.cor.MLR.full[[i]]%>%select(pred_to_keep)
  
  # the two stepAIC parts from above had really good adj R2 so starting with those to see
  # how model building with these predictor set does:
  
  ### v2.1: check which predictors have the highest coefficent of variability among the 40 observations:
  
  v.mean<-sapply(data.temp[,-1], mean, na.rm=T)
  v.sd<-sapply(data.temp[,-1], sd, na.rm=T)
  v.cv<-v.mean/v.sd
  df.cv<-tibble::rownames_to_column(as.data.frame(v.cv), "Attribute")%>%
    rename(var=2)%>%
    arrange(desc(var))
  # the following top n-1 predictors are:
  df.cv$Attribute[1:39]
  # use stepAIC to build a 'full' model using the top n-1:
  fit0 <- lm(term~1,data=data.temp)
  up.model <- paste("~", paste(colnames(data.temp%>%select(df.cv$Attribute[1:39])), collapse=" + "))
  m.step.39.cv <- stepAIC(fit0,direction="both",scope=list(lower=fit0,upper=up.model),trace = FALSE)
  summary(m.step.39.cv) # adj R2 = 0.7909, pval = 2.076e-05
  # this is pretty good but I still want p<n/10 
  # so use leaps to determine best 4:
  var.temp<-strsplit(as.character(m.step.39.cv$terms)[3],split=' + ', fixed=TRUE)[[1]]
  data.temp<-data.temp%>%select(term, var.temp)
  m.leaps.step.cv.39to8to4<-regsubsets(term~.,data=data.temp,nbest=1)
  summary(m.leaps.step.cv.39to8to4)
  rownames(as.data.frame(coef(m.leaps.step.cv.39to8to4, 4)))[-1]
  # now build an lm with these 4:
  data.temp<-select(data.temp, term, rownames(as.data.frame(coef(m.leaps.step.cv.39to8to4, 4)))[-1])
  m.leaps.step.cv.39to8to4<-lm(term~., data=data.temp)
  summary(m.leaps.step.cv.39to8to4) # adj r2 = 0.3339, pval = 0.0009889
  plot(m.leaps.step.cv.39to8to4) # 39, 21, 23 (21 in cooks)
  
  terms.m.step.cv<-names(m.leaps.step.cv.39to8to4$model)[-1]
  
  ### v2.2: check which predictors have the highest variability among the 40 observations:
  
  # need to recreate data.temp:
  
  data.temp<-l.cor.MLR.full[[i]]%>%select(pred_to_keep)
  
  v.var<-sapply(data.temp[,-1], var, na.rm=T)
  df.var<-tibble::rownames_to_column(as.data.frame(v.var), "Attribute")%>%
    rename(var=2)%>%
    arrange(desc(var))
  # the following top n-1 predictors are:
  df.var$Attribute[1:39]
  # build the full model using the top n-1:
  fit0 <- lm(term~1,data=data.temp)
  up.model <- paste("~", paste(colnames(data.temp%>%select(df.var$Attribute[1:39])), collapse=" + "))
  m.step.39.var <- stepAIC(fit0,direction="both",scope=list(lower=fit0,upper=up.model),trace = FALSE)
  summary(m.step.39.var) # adj R2 = 0.9259, pval = 1.01e-08
  # this is amazing but we shouldn't use a model with 22 predictors
  # so use leaps to determine best 4:
  var.temp<-strsplit(as.character(m.step.39.var$terms)[3],split=' + ', fixed=TRUE)[[1]]
  data.temp<-data.temp%>%select(term, var.temp)
  m.leaps.step.var.39to22to4<-regsubsets(term~.,data=data.temp,nbest=1)
  summary(m.leaps.step.var.39to22to4)
  rownames(as.data.frame(coef(m.leaps.step.var.39to22to4, 4)))[-1]
  # now build an lm with these 4:
  data.temp<-select(data.temp, term, rownames(as.data.frame(coef(m.leaps.step.var.39to22to4, 4)))[-1])
  m.leaps.step.var.39to22to4<-lm(term~., data=data.temp)
  summary(m.leaps.step.var.39to22to4) # adj r2 = 0.3967 , pval = 0.0001962
  plot(m.leaps.step.var.39to22to4) # 23, 25, 10, 21 (21 in cooks)
  
  terms.m.step.var<-names(m.leaps.step.var.39to22to4$model)[-1]
  
  
  #
  
  
  
  
  
  #########
  
}


# summary table of models: to do this:

# create a list of model names from gloabl enviornment:

rm(m.simple)

l.m<-names(.GlobalEnv) %>% str_subset(pattern = "^m\\.")

# put the models by their model names into a list:

l.m<-do.call("list",mget(l.m))

# filter the list to those with 5 or less terms (1 for intercept):

l.m<-l.m[sapply(l.m, \(i) ifelse(length(coef(i))<6, T, F))]

tab_model(l.m, dv.labels = names(l.m), file="MLR.tab_model.OLS.I.html")

#













































































#### Categorizing land use ####

# USGS criteria:
# Agricultural sites have >50% agricultural land and ≤5% urban land;
# urban sites have >25% urban and ≤25% agricultural land; 
# undeveloped sites have ≤ 5% urban and ≤ 25% agricultural land; 
# all other combinations of urban, agricultural, and undeveloped lands are classified as mixed

# combine Ag and Pasture into a single landuse for Ag:

df.datalayers<-mutate(df.datalayers, Ag2 = Ag+Pasture)

# set NA to zero

df.datalayers[is.na(df.datalayers)]<-0

# create the land use class column based on USGS critiera:
# adjusted thresholds:

df.datalayers<-df.datalayers%>%
  mutate(USGS.LU.Adjusted = 'Mixed')%>%
  mutate(USGS.LU.Adjusted = case_when(.default = 'Mixed',
                                      Ag2 > .30 & Developed <= .1 ~ 'Agriculture',
                                      Developed > .1 & Ag2 <= .3 ~ 'Urban',
                                      Developed <= .1 & Ag2 <= .1 ~ 'Undeveloped'))

# merge the OLS and Sens slopes and intercepts with this df:

df.datalayers<-left_join(df.datalayers, df.OLS_Sens[,1:5], by = 'Name')

# plot

p<-df.datalayers%>%
  pivot_longer(cols = 15:18, names_to = 'CQ_parameter', values_to = 'Value')%>%
  ggplot(., aes(x=USGS.LU.Adjusted, y=Value, color =USGS.LU.Adjusted ))+
    geom_boxplot(varwidth = TRUE, alpha=0.2)+
    # scale_x_discrete(labels=my_xlab)+
    facet_wrap('CQ_parameter', scales = 'free')+
    stat_compare_means(method = "anova", label.y = max(df.datalayers$Value))+      # Add global p-value
    stat_compare_means(label = "p.signif", method = "t.test",
                       ref.group = "0.5") +
    ggtitle('Adjusted USGS Thresholds using Aggregated Data Layers')

p

#






















#### Categorizing land use (Not run) - experimenting with different data layers ####
# 
# # USGS criteria:
# # Agricultural sites have >50% agricultural land and ≤5% urban land;
# # urban sites have >25% urban and ≤25% agricultural land; 
# # undeveloped sites have ≤ 5% urban and ≤ 25% agricultural land; 
# # all other combinations of urban, agricultural, and undeveloped lands are classified as mixed
# 
# # I have four different datasets to pull land use from:
# # NLCD 2001 and 2019
# # CDL 2008 and 2020
# 
# # The plan is to make two facet plots, one for orginal USGS thresholds and one for adjusted USGS thresholds:
# # facets will be the different CQ parameters with x axis being the different land use catgeory andyaxis beung the parameter value
# # for each land use, color will be used to determine which dataset the values came from
# 
# 
# # In a first pass using these thresholds the number of ag and urban sites wasvery low
# # I will play with these numbers to see what happens
# 
# # merge land use from CDL and NLCD to df.sf.NWIS.keep.2 and add a yearcolumn:
# 
# t1<-left_join(df.sf.NWIS.keep.2, df.NWIS.CDL%>%mutate(LU_source = 'CDL_2020'), by = 'Name')
# t2<-left_join(df.sf.NWIS.keep.2, df.NWIS.CDL.2008%>%mutate(LU_source = 'CDL_2008'), by = 'Name')
# t3<-left_join(df.sf.NWIS.keep.2, l.NWIS.NLCD$`2019`%>%mutate(LU_source = 'NLCD_2019'), by = 'Name')
# t4<-left_join(df.sf.NWIS.keep.2, l.NWIS.NLCD$`2001`%>%mutate(LU_source = 'NLCD_2001'), by = 'Name')
# 
# # merge the CDL and NLCD dfs:
# 
# df.LU<-bind_rows(t1,t2,t3,t4)
# 
# # combine Ag and Pasture into a single landuse for Ag:
# 
# df.LU<-mutate(df.LU, Ag = Ag+Pasture)
# 
# # set NA to zero
# 
# df.LU[is.na(df.LU)]<-0
# 
# # create the land use class column based on USGS critiera:
# 
# # orginal thresholds:
# 
# df.LU<-df.LU%>%
#   mutate(USGS.LU = 'Mixed')%>%
#   mutate(USGS.LU = case_when(.default = 'Mixed',
#                              Ag > .50 & Developed <= .05 ~ 'Agriculture',
#                              Developed > .25 & Ag <= .25 ~ 'Urban',
#                              Developed <= .05 & Ag <= .25 ~ 'Undeveloped')
#   )
# 
# # adjusted thresholds:
# 
# df.LU<-df.LU%>%
#   mutate(USGS.LU.Adjusted = 'Mixed')%>%
#   mutate(USGS.LU.Adjusted = case_when(.default = 'Mixed',
#                              Ag > .30 & Developed <= .1 ~ 'Agriculture',
#                              Developed > .1 & Ag <= .3 ~ 'Urban',
#                              Developed <= .1 & Ag <= .1 ~ 'Undeveloped'))
# 
# # merge the OLS and Sens slopes and intercepts with this df:
# 
# df.LU<-left_join(df.LU, df.OLS_Sens[,1:5], by = 'Name')
# 
# # pivot longer for geom_box + facet:
# 
# data<-df.LU%>%
#   pivot_longer(cols = 16:19, names_to = 'CQ_parameter', values_to = 'Value')%>%
#   mutate(USGS.LU=factor(USGS.LU))
# 
# # make ggplot:
# 
# # for orginal thresholds:
# 
# my_xlab <- paste(levels(factor(df.LU$USGS.LU)),"\n(N=",table(factor(df.LU$USGS.LU)),")",sep="")
# 
# ggplot(data, aes(x=LU_source, y=Value, color =USGS.LU ))+
#   geom_boxplot(varwidth = TRUE, alpha=0.2)+
#   # scale_x_discrete(labels=my_xlab)+
#   facet_wrap('CQ_parameter', scales = 'free')+
#   stat_compare_means(method = "anova", label.y = max(data$Value))+      # Add global p-value
#   stat_compare_means(label = "p.signif", method = "t.test",
#                      ref.group = "0.5") +
#   ggtitle('Adjusted USGS Thresholds using Aggregated Data Layers')
# 
# # for adjusted thresholds:
# 
# my_xlab <- paste(levels(factor(df.LU$USGS.LU.Adjusted)),"\n(N=",table(factor(df.LU$USGS.LU.Adjusted)),")",sep="")
# 
# ggplot(data, aes(x=LU_source, y=Value, color =USGS.LU.Adjusted ))+
#   geom_boxplot(varwidth = TRUE, alpha=0.2)+
#   # scale_x_discrete(labels=my_xlab)+
#   facet_wrap('CQ_parameter', scales = 'free')+
#   stat_compare_means(method = "anova", label.y = max(data$Value))+      # Add global p-value
#   stat_compare_means(label = "p.signif", method = "t.test",
#                      ref.group = "0.5") +
#   ggtitle('Adjusted USGS Thresholds using Aggregated Data Layers')
# 

























#### Grouping CQ curves (stationary, mobilization, dilutionary, complex) ####

# create a list of dataframes for each sites CQ observations:

temp<-df.NWIS.TP_CQ%>%
  rename(Name = site_no)%>%
  filter(Name %in% df.datalayers$Name)%>%
  mutate(log_C = log(result_va), log_Q = log(X_00060_00003), C = result_va, Q = X_00060_00003)%>%
  filter(is.finite(log_C))%>%
  filter(is.finite(log_Q))

l.temp<-temp%>%
  split(., .$Name)

# create lm models for each site:

l.lm.CQ_slopes<-lapply(l.temp, \(i) lm(log_C~log_Q, data=i))

# save the model coef ad pvals:

coef<-tibble::rownames_to_column(as.data.frame(t(sapply(l.lm.CQ_slopes, \(i) summary(i)$coefficients[,1] ))), 'site_no')

# save the pvalues 

pvals<-tibble::rownames_to_column(as.data.frame(t(sapply(l.lm.CQ_slopes, \(i) summary(i)$coefficients[,4] ))), 'site_no')%>%
  rename(I.pval = 2, S.pval = 3)

# merge the two dfs:

m<-left_join(coef,pvals,by='site_no')

# add column for CQ type:

m<-mutate(m, Type = ifelse(S.pval>0.05, 'Stationary', ifelse(log_Q>0, 'Mobilization', 'Dilution')))

# merge labels with plotting df:

temp<-left_join(temp,m%>%select(site_no, Type),by=c('Name'='site_no'))

# create a df for CQplot of all sites with BP analysis:

df_Seg.2<-filter(df_Seg, site %in% temp$Name)%>%
 left_join(.,m%>%select(site_no, Type),by=c('site'='site_no')) 

# make plot:

p<-ggplot(df_Seg.2, aes(x = log(Q_real), y = log(C)))+
  geom_point(aes(color = Type))+
  geom_smooth(method = 'lm')+
  geom_line(aes(x = Q, y = Seg_C), color = 'purple', size = 2)+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'free')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )

# looking at this plot I want to add a fourth CQ type for complex, if the slopes of the BP analysis look widely different. 
# I will start wit hcalcuating the angle between pre-post BP slope and see which sites get signled out:

# add a new column with the angle between the two lines:

df_Seg.2<-df_Seg.2%>%
  mutate(slope_angle=factor(round(atan(abs((Slope2-Slope1)/(1+(Slope2*Slope1)))),1)))

# create color pallete for the slope angle:

hc<-heat.colors(length(unique(df_Seg.2$slope_angle)), rev = T)

# make plot:

p<-ggplot(df_Seg.2, aes(x = log(Q_real), y = log(C)))+
  geom_point(aes(color = Type))+
  scale_color_manual(name = "CQ Type", values = c("red", "blue", "green"))+
  geom_smooth(method = 'lm')+
  new_scale_color() +
  geom_line(aes(x = Q, y = Seg_C), size = 2.5, color = 'black')+
  geom_line(aes(x = Q, y = Seg_C, color = slope_angle), size = 2)+
  scale_color_manual(name = "Slope Angle", values = hc)+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'free')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )

p

# based on this plot, I would chose the following sites as complex:

complex_sites<-unique(df_Seg.2$site)[c(3,16,21,22,29,37)] 

# old complex site (when n=53): c(3,18,24,25,26,35,42,43)

df_Seg.2<-mutate(df_Seg.2, Type = ifelse(site %in% complex_sites, 'Complex', Type))

# make plot:

p<-ggplot(df_Seg.2, aes(x = log(Q_real), y = log(C)))+
  geom_point(aes(color = Type))+
  scale_color_manual(name = "CQ Type", values = c("purple", "red", "blue", "green"))+
  geom_smooth(method = 'lm')+
  geom_line(aes(x = Q, y = Seg_C), color = 'yellow', size = 1.5)+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'free')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )

# I want to add land use as color add number of CAFOs as text. to do this:

# merge the plotting df with the land use for NLCD 2001 and CAFO count and adjusted thresholds:

df_Seg.2<-left_join(df_Seg.2, df.datalayers%>%select(Name, USGS.LU.Adjusted, CAFO_count), by = c('site'='Name'))

# if CAFO count is zero set to NA:

df_Seg.2$CAFO_count[df_Seg.2$CAFO_count==0]<-NA

# plot:

p<-ggplot(df_Seg.2, aes(x = log(Q_real), y = log(C)))+
  geom_point(aes(color = Type), size = 1.5)+
  scale_color_manual(name = "CQ Type", values = c("purple", "red", "blue", "green"))+
  geom_smooth(method = 'lm')+
  geom_line(aes(x = Q, y = Seg_C), color = 'yellow', size = 1.5)+
  # geom_text(aes(x = 2, y = -2, label = CAFO_count), inherit.aes = FALSE, size = 30)+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'free')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )+
  geom_rect(data = df_Seg.2%>%distinct(df_Seg.2$site, .keep_all = T), aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, fill = USGS.LU.Adjusted), alpha = .15)+
  scale_fill_manual(name = "USGS Landuse\n(Adjusted)", values = c("red", "blue","purple", "green"))

p

# plot of trouble sites from MLR analysis:


p<-filter(df_Seg.2, site %in% look_at_sites)%>%
  ggplot(., aes(x = log(Q_real), y = log(C)))+
  geom_point(aes(color = Type), size = 1.5)+
  scale_color_manual(name = "CQ Type", values = c("purple", "red", "blue", "green"))+
  geom_smooth(method = 'lm')+
  geom_line(aes(x = Q, y = Seg_C), color = 'yellow', size = 1.5)+
  # geom_text(aes(x = 2, y = -2, label = CAFO_count), inherit.aes = FALSE, size = 30)+
  facet_wrap(dplyr::vars(site), scales = 'free')+
  # theme(
  #   strip.background = element_blank(),
  #   strip.text.x = element_blank()
  # )+
  geom_rect(data = .%>%distinct(.$site, .keep_all = T), aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, fill = USGS.LU.Adjusted), alpha = .15)+
  scale_fill_manual(name = "USGS Landuse\n(Adjusted)", values = c("red", "blue","purple", "green"))

p

# save(p1, file = 'Processed_Data/p1.Rdata') 
  
# add CQ type to the mapping df with polygon trasperncy based on number of samples:

map.CQ_Type<-df.sf.NWIS.keep%>%
  filter(Name %in% df.datalayers$Name)%>%
  left_join(.,distinct(df_Seg.2, site, .keep_all = T)%>%select(.,c(site, Type, n_sample_rank)),  by = c('Name'='site'))%>%
  select(Name, Type, n_sample_rank)%>%
  arrange(n_sample_rank)%>%
  mutate(NEW = (1/row_number())*40)
  # left_join(.,df.NWIS.TP%>%group_by(site_no)%>%summarise(n=n()), by = c('site'='site_no'))%>%
  # mutate(NEW = case_when(n < 50 ~ .05,
  #                        n >=50 & n < 100 ~ .25,
  #                        n >=100 & n < 500 ~ .5,
  #                        n >=500 ~ .9))
# map:

mapview(map.CQ_Type, zcol = 'Type', alpha.regions = 'NEW')

# map of trouble sites in MLR section:

x<-map.CQ_Type%>%filter(Name %in% look_at_sites)

mapview(x, zcol = 'Type')

######################################################
 #### ~~~~ XY plot of Slopes and Intercepts ~~~~ ####
####################################################

# plot of slopes and intercepts for each CQ type:

m<-m%>%mutate(Type = ifelse(site_no %in% complex_sites, 'Complex', Type))%>%
  left_join(., df.datalayers%>%select(Name, USGS.LU.Adjusted), by = c('site_no'='Name'))

ggplot(m, aes(x=`(Intercept)`, y=log_Q, color = Type))+
  geom_point(size = 2)+
  scale_color_manual(name = "CQ Type", values = c("purple", "red", "blue", "green"))+
  facet_wrap('USGS.LU.Adjusted', scales = 'fixed')
  
# I want to do this with more sites























#### Land Use changes across watersheds (not run) ####

# # Look at changes in NLCD and CDL calculated Ag %land between 2008-2020 for each 53 watershed
# # to do this:
# 
# # pivot longerto getthe land use catgories in a single column:
# 
# df.LU.2<-pivot_longer(df.LU, cols = 6:12, names_to = 'LandUse', values_to = 'values')
# 
# # pivot wider to get the land use source (data layers) in their own columns:
# 
# df.LU.2<-pivot_wider(df.LU.2, names_from = 'LU_source', values_from = 'values')
# 
# # pivot longer to put CDL and NLCD in own columns:
# 
# df.LU.CDL<-pivot_longer(df.LU.2, cols = starts_with('CDL'), names_to = 'CDL', values_to = 'CDL_values')
# df.LU.NLCD<-pivot_longer(df.LU.2, cols = starts_with('NLCD'), names_to = 'NLCD', values_to = 'NLCD_values')
# 
# # group by land use and calculate the difference between the years:
# 
# df.LU.2<-df.LU.2%>%mutate(CDL_diff = abs(round(CDL_2020-CDL_2008,2)),NLCD_diff = abs(round(NLCD_2019-NLCD_2001,2)))%>%
#   arrange(desc(CDL_diff))
# 
























#### Seasonal ANanlysis ####

# is the CQ relaitonship different between seasons at these sites?

# I need to filter down to sites with over 100 samples for this, 20 samples isnt going tocut it:

df_Seg.3<-filter(df_Seg.2, n>100)

# add season column:

df_Seg.3$Season<-getSeason(df_Seg.3$Date)

# plot:

p1<-ggplot(df_Seg.3, aes(x = log(Q_real), y = log(C)))+
  geom_point(aes(color = Season), size = 4)+
  # scale_color_manual(name = "CQ Type", values = c("purple", "red", "blue", "green"))+
  geom_smooth(method = 'lm')+
  geom_line(aes(x = Q, y = Seg_C), color = 'yellow', size = 1.5)+
  # geom_text(aes(x = 2, y = -2, label = CAFO_count), inherit.aes = FALSE, size = 30)+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'free')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank(),
    legend.position = "bottom",
    legend.title=element_text(size=14)
  )+
  geom_rect(data = df_Seg.3%>%distinct(df_Seg.3$site, .keep_all = T), aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, fill = Type), alpha = .15)+
  scale_fill_manual(name = "CQ Type", values = c("red", "blue","purple", "green"))

p1

# save(p1, file = 'Processed_Data/p1.Rdata') 

# 


























############################################
 #### ~~~~ Q Exceedence Proability ~~~~ ####
############################################




























####~~~~ Calcualte EP ~~~~####

# subset the daily flows to the 40 sites:

temp<-filter(df.NWIS.Q, site_no %in% df.datalayers$Name)

# split into lists by site:

temp<-split(temp, f = temp$site_no) 

# sort each dataframe in the list by the flow, add a column for m and n, convert back to a dataframe using bind_rows, and select only the needed columns for the left join (next step):

temp<-bind_rows(lapply(temp, \(i) i[order(i$X_00060_00003,decreasing = T),]%>%mutate(m = 1:n(), n = n())))%>%select(site_no, Date, m, n)

# append the value of M and n for each C-Q observation in the C-Q dataframe:
# need to rename the n column in the left dataframe as well:

df_Seg.2<-left_join(df_Seg.2%>%rename(n_samples = n), temp, by = c("site" = "site_no", "Date" = "Date"))

# calcualte the exceednce proability for Q for each C observation:

df_Seg.2$EP<-round(df_Seg.2$m/(df_Seg.2$n+1), 4)

# plot:

p1<-ggplot(df_Seg.2, aes(x = EP, y = log(C)))+
  geom_point(aes(color = Type))+
  scale_color_manual(name = "Log-Log\nCQ Type", values = c("purple", "red", "blue", "green"))+
  geom_smooth(method = 'lm')+
  # geom_line(aes(x = Q, y = Seg_C), color = 'yellow', size = 1.5)+
  # geom_text(aes(x = .5, y =-2, label = CAFO_count), inherit.aes = FALSE, size=15)+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'free')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )+
  scale_x_reverse()+
  geom_rect(data = df_Seg.2%>%distinct(df_Seg.2$site, .keep_all = T), aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, fill = USGS.LU.Adjusted), alpha = .15)+
  scale_fill_manual(name = "USGS Landuse\n(Adjusted)", values = c("red", "blue","purple", "green"))

p1

save(p1, file = 'Processed_Data/p1.Rdata')

load('Processed_Data/p1.Rdata')

p

#





















####~~~~ Estimate Break point analysis using EP-Q ~~~~####

#  rerun the BP analysis using EP-Q:

l_Seg.EP<-list()
davies.test.matrix.EP<-NULL
EP<-df.NWIS.TP_CQ%>%
  filter(site_no %in% df.datalayers$Name)%>%
  left_join(., df_Seg.2%>%select(site, Date, EP), by = c('site_no'='site', 'sample_dt'='Date'))%>%
  drop_na(result_va, Q_yield)%>%
  rename(site=site_no, Date = sample_dt)
temp.loop<-sort(unique(EP$site))

i<-1

for (i in seq_along(temp.loop)){
  
  tryCatch({
    
    # print the site name for loop debugging:
    
    print(i)
    print(temp.loop[i])
    
    # create a dataframe that will work with segmented. To do this: 
    # filter for the site the loop is in
    # add log transformed C and Q columns, as well as duplicated columns for renamed C and Q
    # filter for real log C and Q values so breakpoint analysis works smoothly:
    
    df<-EP%>%
      filter(site == temp.loop[i])%>%
      mutate(C = result_va)%>%
      mutate(log_C = log(C))%>%
      filter(is.finite(log_C))%>%
      filter(is.finite(EP))
    
    # build a single slope lm for log C and Q. Tis model is also used inthe breakpoint analysis inthenext step:
    
    m<-lm(log_C~EP, df)
    
    # perform breakpoint regression:
    
    m_seg<-segmented(obj = m, npsi = 1)
    
    # perform davies test for constant linear predictor:
    # the results are saved as a string with the the site name and true/false:
    
    x<-paste(temp.loop[i], '-', davies.test(m)$p.val<0.05)
    
    # add the results of davies test to the matrix made prior to this for loop:
    
    davies.test.matrix.EP<-c(davies.test.matrix.EP,x)
    
    # save the breakpoints
    
    bp<-m_seg$psi[1]
    
    # save the slopes: To do this:
    # a conditional statement is needed since sometimes the segmented function wont fit a two slope model at all and will return a object that doesnt work with the slope function used here:
    
    if(length(class(m_seg))==2){
      s<-as.data.frame(slope(m_seg))
    } else{
      s<-NA
    }
    
    # get the intercepts (again conditional statement is needed):
    
    if(length(class(m_seg))==2){
      inter<-as.data.frame(intercept(m_seg))
    } else{
      inter<-NA
    }
    
    
    # get the model fitted data, put in a dataframe, and reformat this dataframe to export out of the loop
    
    if(length(class(m_seg))==2){
      fit <- data.frame(site = df$site, Date = df$Date, Seg_C_EP = fitted(m_seg), I1_EP = inter$Est.[1], I2_EP = inter$Est.[2], Slope1_EP = s[1,1], Slope2_EP = s[2,1], BP_EP = bp)
      result_df<-left_join(df, fit, by = c('site', 'Date'))
    } else{
      fit <- data.frame(site = df$site, Date = df$Date, Seg_C_EP = fitted(m_seg), I1_EP = inter$Est.[1], I2_EP = NA, Slope1_EP = NA, Slope2_EP = NA, BP_EP = NA)
      result_df<-left_join(df, fit, by = c('site', 'Date'))
    }
    
    l_Seg.EP[[i]]<-result_df
    
  }, 
  
  error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
  
}

davies.test.matrix.EP
df.davies.EP<-as.data.frame(davies.test.matrix.EP)%>%
  separate_wider_delim(1, "-", names = c("site", "BP_EP_yes"))%>%
  mutate(across(c(1,2), trimws))
df_Seg.EP<-bind_rows(l_Seg.EP)%>%
  mutate(Seg_EP=EP)
df_Seg.EP<-df_Seg.EP%>%
  left_join(., df.davies.EP, by = 'site')%>%
  mutate(across(c(Seg_C_EP, Seg_EP), ~replace(., BP_EP_yes == 'FALSE', NA)))

# merge the land use, CQ type, data to this df:

df_Seg.EP<-left_join(df_Seg.EP, m%>%select(site_no, Type, USGS.LU.Adjusted), by = c('site'='site_no'))

# plot:

p<-ggplot(df_Seg.EP, aes(x = EP, y = log(C)))+
  geom_point(aes(color = Type), size = 1.5)+
  scale_color_manual(name = "Log-log\nCQ Type", values = c("purple", "red", "blue", "green"))+
  geom_smooth(method = 'lm')+
  new_scale_color()+
  geom_line(aes(x = Seg_EP, y = Seg_C_EP), color = 'yellow', size = 1.5)+
  scale_color_manual(name = "Log(C)~EP-Q\n Breakpoint Analysis", values = "yellow")+
  # geom_text(aes(x = 2, y = -2, label = CAFO_count), inherit.aes = FALSE, size = 30)+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'free_y')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )+
  scale_x_reverse()+
  geom_rect(data = df_Seg.EP%>%distinct(df_Seg.EP$site, .keep_all = T), aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, fill = USGS.LU.Adjusted), alpha = .15)+
  scale_fill_manual(name = "USGS Landuse\n(Adjusted)", values = c("red", "blue","purple", "green"))

p

save(p, file = 'Processed_Data/p1.Rdata')

# I like the idea of using the EP plots to help ID which
# sites are complex. For example, site 22 was deemed complex
# with log-log plots, but in log-EP space it looks similar to
# the other mobilization sites. 























#----------------------------------------#
  ####~~~  Reclassifying CQ type ~~~#### 
   ###~~~  using the EP-Q plots  ~~~### 
#----------------------------------------#

# rerun the df_Seg.2 workflow bt=ut changing the complex_sites vector:
temp<-df.NWIS.TP_CQ%>%
  rename(Name = site_no)%>%
  filter(Name %in% df.datalayers$Name)%>%
  mutate(log_C = log(result_va), log_Q = log(X_00060_00003), C = result_va, Q = X_00060_00003)%>%
  filter(is.finite(log_C))%>%
  filter(is.finite(log_Q))
l.temp<-temp%>%
  split(., .$Name)
l.lm.CQ_slopes<-lapply(l.temp, \(i) lm(log_C~log_Q, data=i))
coef<-tibble::rownames_to_column(as.data.frame(t(sapply(l.lm.CQ_slopes, \(i) summary(i)$coefficients[,1] ))), 'site_no')
pvals<-tibble::rownames_to_column(as.data.frame(t(sapply(l.lm.CQ_slopes, \(i) summary(i)$coefficients[,4] ))), 'site_no')%>%
  rename(I.pval = 2, S.pval = 3)
m<-left_join(coef,pvals,by='site_no')
m<-mutate(m, Type = ifelse(S.pval>0.05, 'Stationary', ifelse(log_Q>0, 'Mobilization', 'Dilution')))
temp<-left_join(temp,m%>%select(site_no, Type),by=c('Name'='site_no'))
df_Seg.2<-filter(df_Seg, site %in% temp$Name)%>%
  left_join(.,m%>%select(site_no, Type),by=c('site'='site_no')) 
df_Seg.2<-df_Seg.2%>%
  mutate(slope_angle=factor(round(atan(abs((Slope2-Slope1)/(1+(Slope2*Slope1)))),1)))
hc<-heat.colors(length(unique(df_Seg.2$slope_angle)), rev = T)
complex_sites<-unique(df_Seg.2$site)[c(3,16,21,37)] 
df_Seg.2<-mutate(df_Seg.2, Type = ifelse(site %in% complex_sites, 'Complex', Type))
df_Seg.2<-left_join(df_Seg.2, df.datalayers%>%select(Name, USGS.LU.Adjusted, CAFO_count), by = c('site'='Name'))
df_Seg.2$CAFO_count[df_Seg.2$CAFO_count==0]<-NA

# plot:

p<-ggplot(df_Seg.2, aes(x = log(Q_real), y = log(C)))+
  geom_point(aes(color = Type), size = 1.5)+
  scale_color_manual(name = "CQ Type", values = c("purple", "red", "blue", "green"))+
  geom_smooth(method = 'lm')+
  geom_line(aes(x = Q, y = Seg_C), color = 'yellow', size = 1.5)+
  # geom_text(aes(x = 2, y = -2, label = CAFO_count), inherit.aes = FALSE, size = 30)+
  facet_wrap(dplyr::vars(n_sample_rank), scales = 'free')+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )+
  geom_rect(data = df_Seg.2%>%distinct(df_Seg.2$site, .keep_all = T), aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, fill = USGS.LU.Adjusted), alpha = .15)+
  scale_fill_manual(name = "USGS Landuse\n(Adjusted)", values = c("red", "blue","purple", "green"))

p

#

























































# finally save image to workspace:

# save.image(file = 'Processed_Data/NWIS.Rdata')

load('Processed_Data/NWIS.Rdata')




